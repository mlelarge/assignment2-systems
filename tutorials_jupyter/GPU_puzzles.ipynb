{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57624bc4-fb33-40cf-bdf3-5aa25f89d22f",
   "metadata": {},
   "source": [
    "# Programming on GPUs\n",
    "\n",
    "The goal of this notebook is to learn the basics of programming on GPUs. We start with [Numba](https://numba.pydata.org/) a just-in-time compiler for Python and then move on to [Triton](https://openai.com/index/triton/) an open-source Python-like programming language developed by OpenAI.\n",
    "\n",
    "You will probably need to learn a lot of new concepts but this notebook will ask you to code in an interactive way with minimal presentation of the core concepts. If you are stuck ask for help to your favorite chat without asking for the exact solution.\n",
    "\n",
    "Sources: Nvidia [CUDA documentation](https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html) and [GPU-Puzzles](https://github.com/srush/GPU-Puzzles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e256bcb-6387-4811-9959-f448426b42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cd6e1d-a546-4cef-b9a4-d0f42e24ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", category=numba.NumbaPerformanceWarning, module=\"numba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929cee2-ac79-4576-9c9c-4442ef7123ff",
   "metadata": {},
   "source": [
    "## Main Concepts\n",
    "\n",
    "- **Streaming Multiprocessor (SM)** GPU's unit of computation \n",
    "- **Thread** smallest unit of execution\n",
    "- **Block or Thread Block** a group of threads guaranteed to run on a single SM \n",
    "- **Grid** Thread blocks are organized into a grid\n",
    "- **Warp** Within a thread block, threads are organized into groups of 32 threads called warps. A warp executes the kernel code in a Single-Instruction Multiple-Threads (SIMT) paradigm. In SIMT, all threads in the warp are executing the same kernel code.\n",
    "\n",
    "Let's start! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2911024-cc0b-46e9-9685-e1e6710f5920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 11., 12., 13.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_spec(a):\n",
    "    return a + 10\n",
    "\n",
    "# Size of our array\n",
    "SIZE = 4\n",
    "\n",
    "# Create input and output arrays\n",
    "a = np.arange(SIZE, dtype=np.float32)  # [0, 1, 2, 3]\n",
    "out = np.zeros(SIZE, dtype=np.float32)\n",
    "\n",
    "map_spec(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7924d20-47ab-4310-98fe-134781b5f651",
   "metadata": {},
   "source": [
    "Now we want to code this so that each thread adds 10 to one element of the array using Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a463ab-87e8-4a84-9c59-793297551b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    [0. 1. 2. 3.]\n",
      "Output:   [10. 11. 12. 13.]\n",
      "Expected: [10. 11. 12. 13.]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def map_kernel(out, a):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # Each thread adds 10 to one element\n",
    "    # your code here\n",
    "    out[i] = a[i] + 10\n",
    "\n",
    "\n",
    "# Copy arrays to GPU\n",
    "a_device = cuda.to_device(a)\n",
    "out_device = cuda.to_device(out)\n",
    "\n",
    "# kernel[grid, block](args)\n",
    "# Launch kernel: grid = 1 block, block = SIZE threads\n",
    "map_kernel[1, SIZE](out_device, a_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Input:    {a}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e2d63-6175-4c4b-95c2-c425a0804ce7",
   "metadata": {},
   "source": [
    "Let's move to vector addition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b8fcdf-cd27-4bb3-becd-60661a021407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zip_spec(a, b):\n",
    "    return a + b\n",
    "\n",
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "zip_spec(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90295c2d-1514-46f5-a755-10cd76e7e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a:  [0 1 2 3]\n",
      "Input b:  [0 1 2 3]\n",
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def zip_kernel(out, a, b):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # your code here\n",
    "    out[i] = a[i] + b[i]\n",
    "\n",
    "# A function to move vectors on device\n",
    "def init_pb(a=a, b=b, out=out):\n",
    "    a_device = cuda.to_device(a)\n",
    "    b_device = cuda.to_device(b)\n",
    "    out_device = cuda.to_device(out)\n",
    "    return a_device, b_device, out_device\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "# Launch kernel: 1 block, SIZE threads\n",
    "zip_kernel[1, SIZE](out_device, a_device, b_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Input a:  {a}\")\n",
    "print(f\"Input b:  {b}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b00ec4-6aa5-44e8-8a79-0e9bb0a6ac67",
   "metadata": {},
   "source": [
    "What happens if you increase the number of threads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ed1ebc-96fd-4d7c-afc2-172f47b48840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "NUM_TRHEADS = 2*SIZE\n",
    "zip_kernel[1, NUM_TRHEADS](out_device, a_device, b_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b977472-73cf-49cf-a808-1b79ffce48d5",
   "metadata": {},
   "source": [
    "Still working but unsafe (due to out-of-bounds access). Add a guard clause to prevent out-of-bounds access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47d9b2f-d542-4286-aa78-19152ce85474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# CUDA kernel with Guard\n",
    "@cuda.jit\n",
    "def zip_guard_kernel(out, a, b, size):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # your code here\n",
    "    if i < size:\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "NUM_TRHEADS = 2*SIZE\n",
    "zip_guard_kernel[1, NUM_TRHEADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb57cf-877f-4c2b-839a-ebf1f5ef877f",
   "metadata": {},
   "source": [
    "Let's move to matrices now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35e349e-0d40-43a0-b409-79ad7836d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 11, 12, 13],\n",
       "       [14, 15, 16, 17],\n",
       "       [18, 19, 20, 21],\n",
       "       [22, 23, 24, 25]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))\n",
    "out = map_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40147ee-56f6-4479-9eb1-58e8327fa17c",
   "metadata": {},
   "source": [
    "So far, we used 1-D **Thread Block** but **Thread Blocks** can be organized in 2-D or 3-D. These dimensions can simplify mapping of individual threads to units of work or data items.\n",
    "Below, you need to use a 2-D **Thread Block**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948815aa-6a9e-428d-b472-f9858ef1dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[10 11 12 13]\n",
      " [14 15 16 17]\n",
      " [18 19 20 21]\n",
      " [22 23 24 25]]\n",
      "Expected: [[10 11 12 13]\n",
      " [14 15 16 17]\n",
      " [18 19 20 21]\n",
      " [22 23 24 25]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def map_2d_kernel(out, a, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    # your code here\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,j] + 10\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "TRHEAD_BLOCK = (SIZE, SIZE)\n",
    "map_2d_kernel[1, TRHEAD_BLOCK](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9525d76-f77e-4d89-9a36-9081e2f4f135",
   "metadata": {},
   "source": [
    "Broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8bc22dc-5282-4687-a058-c14441863971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [1, 2, 3, 4],\n",
       "       [2, 3, 4, 5],\n",
       "       [3, 4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(SIZE).reshape(SIZE, 1)\n",
    "b = np.arange(SIZE).reshape(1, SIZE)\n",
    "out = a + b\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858dcc2a-20ed-472c-8125-7621ea5c8c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_kernel(out, a, b, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    #your code here\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREAD_BLOCK = (2*SIZE, 3*SIZE)\n",
    "broadcast_kernel[1, THREAD_BLOCK](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc96aa9-bacd-4355-b86f-47d58c779687",
   "metadata": {},
   "source": [
    "So far, we did not use **grid** but like **Thread Block**, grids may be 1, 2, or 3 dimensional. Below use a 2D grid with a single thread per block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed23d449-fcd6-44d1-a150-ea58779f1798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x \n",
    "    j = cuda.blockIdx.y\n",
    "    # your code here\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "# 1 thread per block, 2D grid\n",
    "THREADS = 1\n",
    "GRID = (SIZE, SIZE)\n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879131b7-cabf-47bc-ae67-0eedb3e1cba2",
   "metadata": {},
   "source": [
    "Below, you need to use a 1D grid with a 1D Thread block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2281d6cd-4cc4-45f9-859c-e3dda4586a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [2 0 0 0]\n",
      " [3 0 0 0]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  False\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    # your code here\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE\n",
    "GRID = SIZE\n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71de175-064f-4db6-9f9b-2ec9e397894d",
   "metadata": {},
   "source": [
    "And now, use small 2D grid and blocks.\n",
    "Here we have `cuda.blockDim.x = cuda.blockDim.y =2 ` and sinc `SIZE//2 = 2`, you need to compute i,j as below:\n",
    "\n",
    "| blockIdx.x | blockIdx.y  | threadIdx.x | threadIdx.y | **i** | **j** | Computes |\n",
    "|------------|---------------|-------------|-------------|-------|-------|----------|\n",
    "| 0 | 0 | 0 | 0 | **0** | **0** | out[0,0] |\n",
    "| 0 | 0 | 1 | 0 | **1** | **0** | out[1,0] |\n",
    "| 0 | 0 | 0 | 1 | **0** | **1** | out[0,1] |\n",
    "| 0 | 0 | 1 | 1 | **1** | **1** | out[1,1] |\n",
    "| 1 | 0 | 0 | 0 | **2** | **0** | out[2,0] |\n",
    "| 1 | 0 | 1 | 0 | **3** | **0** | out[3,0] |\n",
    "| 0 | 1 | 0 | 0 | **0** | **2** | out[0,2] |\n",
    "| 0 | 1 | 1 | 1 | **1** | **3** | out[1,3] |\n",
    "| 1 | 1 | 0 | 0 | **2** | **2** | out[2,2] |\n",
    "| 1 | 1 | 1 | 1 | **3** | **3** | out[3,3] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea2afc6-c4c4-4276-a79e-190ade51d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    # your code here\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = (SIZE//2 , SIZE//2)\n",
    "GRID = (SIZE//2, SIZE//2)  \n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e0e2a-602b-4d0b-b4b0-abef682330dc",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "<div>\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-programming-guide/_images/gpu-cpu-system-diagram.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "- **Dynamic random-access memory DRAM** DRAM attached to the GPU is called **global memory**, because it is accessible to all SMs in the GPU. The DRAM attached to the CPU(s) is called system memory or host memory.\n",
    "- On-Chip Memory in GPUs: **Shared Memory** and **Registers**. Programmers use this shared memory as a manually managed cache. Threads have access to private **registers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7633ae72-85e7-4196-bde1-fd187b750b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  3.,  6.,  9., 12., 15., 18.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool_spec(a):\n",
    "    out = np.zeros(a.shape)\n",
    "    for i in range(a.shape[0]):\n",
    "        out[i] = a[max(i - 2, 0) : i + 1].sum()\n",
    "    return out\n",
    "\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE)\n",
    "out = pool_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac15df9-9dcd-4493-bf31-571f774801a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Expected: [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Correct:  True\n",
      "number of access to global memory: 1 + 2 + 2 threads x 3 reads = 9 global reads per block -> 18 global reads in total\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def pool_kernel(out, a, size):\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        # Manually compute sum - can't use slicing in CUDA!\n",
    "        temp_sum = 0.0\n",
    "        for k in range(max(i - 2, 0), i + 1):\n",
    "            temp_sum += a[k]  # Use global memory\n",
    "        out[i] = temp_sum\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE//2\n",
    "GRID = (2,1)  \n",
    "pool_kernel[GRID, THREADS](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")\n",
    "r = 3\n",
    "print(f\"number of access to global memory: 1 + 2 + {THREADS-2} threads x {r} reads = {1+2+(THREADS-2)*r} global reads per block -> {2*(1+2+(THREADS-2)*r)} global reads in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9017cc-7569-48d5-89e0-fc931f89126a",
   "metadata": {},
   "source": [
    "So now, we want to get a lower number of access to global memory by using shared memory.\n",
    "\n",
    "```\n",
    "Memory hierarchy:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Global Memory (a, out)             ‚îÇ  ‚Üê ALL threads can access\n",
    "‚îÇ  - Slow                             ‚îÇ\n",
    "‚îÇ  - Accessible across all blocks     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì                    ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ Block 0 ‚îÇ          ‚îÇ Block 1 ‚îÇ\n",
    "   ‚îÇ Shared  ‚îÇ          ‚îÇ Shared  ‚îÇ      ‚Üê Only threads in THIS block\n",
    "   ‚îÇ Memory  ‚îÇ          ‚îÇ Memory  ‚îÇ         can access\n",
    "   ‚îÇ (fast)  ‚îÇ          ‚îÇ (fast)  ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "The method below allows for 8+4=12 global reads in total.\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Global:      [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "Block 0 loads:              Block 1 loads:\n",
    "        ‚Üì                          ‚Üì\n",
    "Shared: [0, 0, 0, 1, 2, 3] Shared: [2, 3, 4, 5, 6, 7]\n",
    "         ‚îî‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        halo    main data          halo    main data\n",
    "```\n",
    "\n",
    "Each block can only have a constant amount of shared memory that threads in that block can read and write to. This needs to be a literal python constant not a variable. After writing to shared memory you need to call `cuda.syncthreads` to ensure that threads do not cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92a76dde-423b-459d-8027-4eb99c3a0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Expected: [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "TPB = 4  # Threads per block\n",
    "SharedMem = TPB + 2 # cannot be computed at runtime\n",
    "@cuda.jit\n",
    "def pool_kernel_shared(out, a, size):\n",
    "    # Allocate shared memory with HALO (extra elements for boundary)\n",
    "    # Need TPB + 2 extra elements (for the 2-element lookback)\n",
    "    shared = cuda.shared.array(SharedMem, numba.float32)\n",
    "    \n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    local_i = cuda.threadIdx.x\n",
    "    \n",
    "    # Each thread loads its own element into shared memory (offset by 2 for halo)\n",
    "    if i < size:\n",
    "        shared[local_i + 2] = a[i]\n",
    "    \n",
    "    # First 2 threads also load the HALO (left boundary elements)\n",
    "    if local_i == 0:\n",
    "        # Load 2 elements before the block starts\n",
    "        start_idx = cuda.blockIdx.x * cuda.blockDim.x\n",
    "        if start_idx >= 2:\n",
    "            shared[1] = a[start_idx - 1]\n",
    "            shared[0] = a[start_idx - 2]\n",
    "        else:\n",
    "            shared[1] = 0.0\n",
    "            shared[0] = 0.0 # Padding for out-of-bounds\n",
    "            \n",
    "    # Wait for all threads to finish loading\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Now compute using shared memory\n",
    "    if i < size:\n",
    "        temp_sum = 0.0\n",
    "        # Look back up to 2 elements in shared memory\n",
    "        for k in range(max(0, 3 - (i + 1)), 3):  # At most 3 elements\n",
    "            temp_sum += shared[local_i + 2 - (2 - k)]\n",
    "        out[i] = temp_sum\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "\n",
    "GRID = (SIZE // TPB, 1)  # (2, 1) for SIZE=8, TPB=4\n",
    "pool_kernel_shared[GRID, TPB](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad89325-3bfe-4bd0-8905-ef6b43beccb8",
   "metadata": {},
   "source": [
    "Now, we need to implement the dot product. Serially, the code is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce013ae9-bb0e-42e5-a36e-2f3e30c28da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(140.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_spec(a,b):\n",
    "    tot = 0\n",
    "    for i in range(len(a)):\n",
    "        tot += a[i]*b[i]\n",
    "    return tot\n",
    "\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "dot_spec(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c68f3-7961-4ed5-ba8a-db7ea7a5e8a2",
   "metadata": {},
   "source": [
    "In parallel, the problem is a lot harder. Here is a a tree-based summation algorithm:\n",
    "\n",
    "![](https://www.cs.uaf.edu/2012/fall/cs441/lecture/tree_sum_16td.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d852a2b3-62a3-4bbb-b0e8-98d0900447bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(140.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_tree(a,b):\n",
    "    size = len(a)\n",
    "    shared_mem = np.zeros(size)\n",
    "    for i in range(size):\n",
    "       shared_mem[i] = a[i]*b[i]\n",
    "    stride = size // 2\n",
    "    while stride > 0:\n",
    "        for i in range(stride):\n",
    "            shared_mem[i] += shared_mem[i+stride]\n",
    "        stride //=2\n",
    "    return shared_mem[0]\n",
    "dot_tree(a,b)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab60ed8-d6d8-4fb8-ab1d-eb680df8a981",
   "metadata": {},
   "source": [
    "Now you need to code this algorithm in numba. You have 256 threads per block and sufficient blocks to have more threads than the size of your input vectors `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8a757d9-7dac-44ae-a18d-ab3a01090926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threads per block: 256\n",
      "blocks per grid: 4\n"
     ]
    }
   ],
   "source": [
    "SIZE = 800\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (SIZE + threads_per_block - 1) // threads_per_block\n",
    "print(f\"threads per block: {threads_per_block}\")\n",
    "print(f\"blocks per grid: {blocks_per_grid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903affae-6e73-441f-a126-5fa6d42fa1d6",
   "metadata": {},
   "source": [
    "For each block, you can implement the tree-based summation algorithm by first creating a shared memory of size 256 containing the `a[i]*b[i]` with the `i` associated witht the block thread. Then suming it accross the block. The last step consists in adding all the intermediate results: each block adds its result. For this last step, you might want to use `cuda.atomic.add` see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaffa23-43cd-4bcf-aa9d-d291dc1f0df5",
   "metadata": {},
   "source": [
    "### The Problem Without Atomics\n",
    "When you write out[0] = out[0] + value, it's actually 3 separate steps:\n",
    "```\n",
    "# out[0] = out[0] + value breaks down to:\n",
    "1. READ:   temp = out[0]      # Read current value\n",
    "2. MODIFY: temp = temp + value # Add to it\n",
    "3. WRITE:  out[0] = temp       # Write back\n",
    "```\n",
    "\n",
    "**With multiple threads, these steps can interleave and lose updates:**\n",
    "```\n",
    "Initial: out[0] = 0\n",
    "\n",
    "Thread A (Block 0):              Thread B (Block 1):\n",
    "1. READ: temp_A = 0             \n",
    "2. MODIFY: temp_A = 0 + 5       \n",
    "                                 1. READ: temp_B = 0      ‚Üê Still sees 0!\n",
    "3. WRITE: out[0] = 5            \n",
    "                                 2. MODIFY: temp_B = 0 + 3 ‚Üê Uses old value!\n",
    "                                 3. WRITE: out[0] = 3      ‚Üê Overwrites 5!\n",
    "\n",
    "Final: out[0] = 3  ‚ùå Should be 8!\n",
    "```\n",
    "\n",
    "### What Atomics Do\n",
    "\n",
    "`cuda.atomic.add(out, 0, value)` **locks the memory location** so the entire operation completes before another thread can access it:\n",
    "```\n",
    "Initial: out[0] = 0\n",
    "\n",
    "Thread A (Block 0):              Thread B (Block 1):\n",
    "üîí LOCK out[0]\n",
    "1. READ: temp_A = 0             \n",
    "2. MODIFY: temp_A = 0 + 5       \n",
    "3. WRITE: out[0] = 5            \n",
    "üîì UNLOCK out[0]\n",
    "                                 üîí LOCK out[0]  ‚Üê Must wait for unlock\n",
    "                                 1. READ: temp_B = 5      ‚Üê Sees updated value!\n",
    "                                 2. MODIFY: temp_B = 5 + 3\n",
    "                                 3. WRITE: out[0] = 8\n",
    "                                 üîì UNLOCK out[0]\n",
    "\n",
    "Final: out[0] = 8  ‚úÖ Correct!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a6919a7-95f1-4887-b86d-667784b92405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA result: 170346800.0\n",
      "NumPy result: 170346800.0\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def dot_kernel_numba(a, b, out, size):\n",
    "    shared = cuda.shared.array(256, numba.float32)\n",
    "    # your code here\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    local_i = cuda.threadIdx.x\n",
    "    \n",
    "    if i < size:\n",
    "        shared[local_i] = a[i] * b[i]\n",
    "    else:\n",
    "        shared[local_i] = 0.0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if local_i < stride:\n",
    "            shared[local_i] += shared[local_i + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "    \n",
    "    if local_i == 0:\n",
    "        cuda.atomic.add(out, 0, shared[0]) # To avoid RACE CONDITION! Multiple blocks write to out[0] see below\n",
    "\n",
    "\n",
    "expected = np.dot(a, b)\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like([expected]))\n",
    "\n",
    "size = a_device.shape[0]\n",
    "dot_kernel_numba[blocks_per_grid, threads_per_block](a_device, b_device, out_device, size)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "print(f\"CUDA result: {result[0]}\")\n",
    "print(f\"NumPy result: {expected}\")\n",
    "print(f\"Match: {np.allclose(result[0], expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95acae5-ede3-4413-8ad6-3997374979dd",
   "metadata": {},
   "source": [
    "## Numba: Grid and Block Dimensions\n",
    "\n",
    "**Key concepts:**\n",
    "- `kernel[grid, block](args)` - Launch syntax\n",
    "- **Grid** = `(blocks_x, blocks_y, blocks_z)` - How many blocks\n",
    "- **Block** = `(threads_x, threads_y, threads_z)` - How many threads per block\n",
    "- Total threads = `grid_x √ó grid_y √ó grid_z √ó block_x √ó block_y √ó block_z`\n",
    "\n",
    "---\n",
    "\n",
    "## Triton: Program Grid\n",
    "\n",
    "In **Triton**, you specify a **program grid** and work with **program IDs**.\n",
    "\n",
    "**Key concepts:**\n",
    "- `kernel[grid](args, BLOCK_SIZE=...)` - Launch syntax\n",
    "- **Grid** = `(programs_x, programs_y, programs_z)` - Number of program instances\n",
    "- **No explicit block/thread dimensions** - Triton handles threading automatically\n",
    "- Work with **blocks of data** using `tl.arange()` and vectorized operations\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Aspect | Numba CUDA | Triton |\n",
    "|--------|------------|--------|\n",
    "| **Launch syntax** | `kernel[grid, block](args)` | `kernel[grid](args, BLOCK=...)` |\n",
    "| **Grid represents** | Number of **blocks** | Number of **programs** |\n",
    "| **Block/Thread control** | Explicit: `(tx, ty, tz)` per block | Abstracted: work on data blocks |\n",
    "| **Thread indexing** | Manual: `blockIdx`, `threadIdx` | Automatic: `tl.program_id()` + `tl.arange()` |\n",
    "| **Typical grid** | `(n_blocks_x, n_blocks_y, n_blocks_z)` | `(n_programs_x, n_programs_y, n_programs_z)` |\n",
    "| **Typical block** | `(threads_x, threads_y, threads_z)` | N/A (implicit in `BLOCK_SIZE`) |\n",
    "| **Memory access** | Per-thread indexing | Vectorized block operations |\n",
    "| **Abstraction level** | Low-level (like CUDA C) | High-level (compiler optimizes) |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "- **Numba CUDA**: You think in terms of **blocks of threads** (2-level hierarchy: grid ‚Üí blocks ‚Üí threads)\n",
    "- **Triton**: You think in terms of **programs operating on data blocks** (1-level: grid ‚Üí programs, with implicit vectorization)\n",
    "\n",
    "Triton is conceptually **one level of \"blocks\"** in CUDA terms - each Triton program is roughly equivalent to a CUDA block, but Triton automatically handles the thread-level parallelism within that program.\n",
    "\n",
    "## Practical Example: Vector Addition\n",
    "\n",
    "Below is the solution of a puzzle above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342ddee6-0796-493d-99ea-93a44ed3bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# CUDA kernel with Guard\n",
    "@cuda.jit\n",
    "def zip_guard_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "SIZE = 1000\n",
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=out)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (SIZE + threads_per_block - 1) // threads_per_block\n",
    "zip_guard_kernel[blocks_per_grid, threads_per_block](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "830121a7-ecf5-4272-8737-3c9ac30b79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1173350e-99b7-4896-9106-d2dff7765435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def zip_guard_triton(a_ptr, b_ptr, out_ptr, n, BLOCK_SIZE: tl.constexpr):\n",
    "    # Triton uses program_id (block index) instead of explicit blockIdx/threadIdx\n",
    "    # Each \"program\" processes BLOCK_SIZE elements at once (vectorized)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Triton computes offsets for a *vector* of BLOCK_SIZE elements\n",
    "    # Unlike Numba where each thread processes 1 element,\n",
    "    # Triton processes multiple elements per program instance\n",
    "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Triton uses mask-based guards for vectorized operations\n",
    "    # instead of scalar if-statements (if i < size)\n",
    "    mask = offset < n\n",
    "    \n",
    "    # Vectorized load: loads BLOCK_SIZE elements at once with mask\n",
    "    # Numba loads scalar: a[i]\n",
    "    a = tl.load(a_ptr + offset, mask=mask)\n",
    "    b = tl.load(b_ptr + offset, mask=mask)\n",
    "    \n",
    "    # Vectorized computation (same as Numba but on vectors)\n",
    "    c = a + b\n",
    "    \n",
    "    # Vectorized store with mask (vs. scalar store in Numba)\n",
    "    tl.store(out_ptr + offset, c, mask=mask)\n",
    "\n",
    "\n",
    "# Triton works directly with PyTorch tensors (no manual copy_to_host)\n",
    "# Numba requires explicit device memory management (init_pb, copy_to_host)\n",
    "a = torch.randn(SIZE, device=get_device())\n",
    "b = torch.randn(SIZE, device=get_device())\n",
    "out = torch.empty_like(a)\n",
    "\n",
    "\n",
    "# Launch syntax differences:\n",
    "# - BLOCK_SIZE is a compile-time constant (tl.constexpr) for optimization\n",
    "# - Only need to specify grid dimensions (not threads_per_block)\n",
    "# - Triton auto-vectorizes within each program\n",
    "BLOCK_SIZE = 256\n",
    "grid = (triton.cdiv(SIZE, BLOCK_SIZE),)  # Only grid size, not block size\n",
    "zip_guard_triton[grid](a, b, out, SIZE, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Correct:  {np.allclose(out.cpu().numpy(), expected.cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b1fb5-ff7a-4730-8283-5c8beb9ef55b",
   "metadata": {},
   "source": [
    "Now you need to code the dot product with triton. You can use [`tl.sum` ](https://triton-lang.org/main/python-api/generated/triton.language.sum.html) and [`tl.atomic_add`](https://triton-lang.org/main/python-api/generated/triton.language.atomic_add.html#triton.language.atomic_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a72ef4-0b17-47e6-90e6-837363b56613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton result: 285.0\n",
      "PyTorch result: 285.0\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def dot_kernel(\n",
    "    a_ptr,      # Pointer to first input vector\n",
    "    b_ptr,      # Pointer to second input vector  \n",
    "    out_ptr,    # Pointer to output scalar\n",
    "    size,       # Size of vectors\n",
    "    BLOCK_SIZE: tl.constexpr,  # Elements per program\n",
    "):\n",
    "    # Program ID (analogous to blockIdx.x)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Compute offsets for this program's block\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Mask for boundary handling\n",
    "    mask = offsets < size\n",
    "    \n",
    "    # Load data \n",
    "    a = tl.load(a_ptr + offsets, mask=mask)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    products = a * b\n",
    "    \n",
    "    # Reduce sum within this block (automatic parallel reduction!)\n",
    "    block_sum = tl.sum(products)\n",
    "    \n",
    "    # Atomic add to output (single thread per block does this)\n",
    "    tl.atomic_add(out_ptr, block_sum)\n",
    "\n",
    "\n",
    "def dot_triton(a, b):\n",
    "    \"\"\"Wrapper function to launch the kernel\"\"\"\n",
    "    # Allocate output\n",
    "    out = torch.zeros(1, device=a.device, dtype=a.dtype)\n",
    "    \n",
    "    # Grid and block configuration\n",
    "    size = a.shape[0]\n",
    "    BLOCK_SIZE = 256  \n",
    "    grid = (triton.cdiv(size, BLOCK_SIZE),)\n",
    "    \n",
    "    # Launch kernel\n",
    "    dot_kernel[grid](a, b, out, size, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# Usage example\n",
    "SIZE = 10\n",
    "a = torch.arange(SIZE, dtype=torch.float32, device=get_device())\n",
    "b = torch.arange(SIZE, dtype=torch.float32, device=get_device())\n",
    "\n",
    "result_triton = dot_triton(a, b)\n",
    "result_torch = torch.dot(a, b)\n",
    "\n",
    "print(f\"Triton result: {result_triton.item()}\")\n",
    "print(f\"PyTorch result: {result_torch.item()}\")\n",
    "print(f\"Match: {torch.allclose(result_triton, result_torch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff070e-1cd8-47f7-9748-afb130d1681a",
   "metadata": {},
   "source": [
    "Now, you need to code the softmax function for a batch of vectors z of size `bs x d` we want to compute `torch.softmax(z, axis=1)`.\n",
    "**Visual:**\n",
    "```\n",
    "Memory: [a, b, c, d, e, f, g, h, i, j, k, l]\n",
    "Shape (3, 4), stride (4, 1):\n",
    "  [[a, b, c, d],    ‚Üê skip 1 for next col, skip 4 for next row\n",
    "   [e, f, g, h],\n",
    "   [i, j, k, l]]\n",
    "```\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd3b901c-56ee-4236-b561-93a2e81d80e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contiguous 2D tensor (3√ó4)\n",
    "x = torch.randn(3, 4)\n",
    "x.stride()  # (4, 1)\n",
    "# - Move to next row: skip 4 elements\n",
    "# - Move to next column: skip 1 element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa9c0f44-1ac9-43a3-b9ef-9d21e6b895a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposed (now 4√ó3)\n",
    "y = x.t()\n",
    "y.stride()  # (1, 4)\n",
    "# - Move to next row: skip 1 element (was column)\n",
    "# - Move to next column: skip 4 elements (was row)\n",
    "# Note: y shares memory with x, just different access pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e4efb-5ddc-4b65-87a4-b647a6cba214",
   "metadata": {},
   "source": [
    "**Why it matters:**\n",
    "For our softmax function, we assume that each program can process one full row independently. The first task will be to locate the start of the corresponding row in memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "380315a5-3677-4a59-be40-45f942bc6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def triton_softmax_kernel(x_ptr, y_ptr, x_row_stride, y_row_stride, num_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    assert num_cols <= BLOCK_SIZE\n",
    "    # Process each row independently\n",
    "    row_idx = tl.program_id(0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    # Read from global memory\n",
    "    x_start_ptr = x_ptr + row_idx * x_row_stride\n",
    "    x_ptrs = x_start_ptr + col_offsets\n",
    "    x_row = tl.load(x_ptrs, mask=col_offsets < num_cols, other=float(\"-inf\"))\n",
    "    # Compute\n",
    "    x_row = x_row - tl.max(x_row, axis=0)\n",
    "    numerator = tl.exp(x_row)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    y_row = numerator / denominator\n",
    "    # Write back to global memory\n",
    "    y_start_ptr = y_ptr + row_idx * y_row_stride\n",
    "    y_ptrs = y_start_ptr + col_offsets\n",
    "    tl.store(y_ptrs, y_row, mask=col_offsets < num_cols)\n",
    "\n",
    "def triton_softmax(x: torch.Tensor):\n",
    "    x = x.contiguous()\n",
    "    # Allocate output tensor\n",
    "    y = torch.empty_like(x)\n",
    "    # Determine grid\n",
    "    M, N = x.shape                          # Number of rows x number of columns\n",
    "    block_size = triton.next_power_of_2(N)  # Each block contains all the columns\n",
    "    num_blocks = M                          # Each block is a row\n",
    "    # Launch kernel\n",
    "    triton_softmax_kernel[(M,)](\n",
    "        x_ptr=x, y_ptr=y,\n",
    "        x_row_stride=x.stride(0), y_row_stride=y.stride(0),\n",
    "        num_cols=N, BLOCK_SIZE=block_size\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1735d03-3bfa-41b5-8b4c-67235974d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device=get_device())\n",
    "y_triton = triton_softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89635810-32e9-4198-8a3e-fb168eed7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_triton = triton_softmax(x.t())\n",
    "y_torch = torch.softmax(x.t(), axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddc34a21-9928-44d1-820b-4fdb1518a250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = get_device()\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch\"],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    stream = getattr(torch, DEVICE.type).Stream()\n",
    "    getattr(torch, DEVICE.type).set_stream(stream)\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: triton_softmax(x))\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cefcdf-e2b0-4332-b661-caeae619b06b",
   "metadata": {},
   "source": [
    "## Block pointers\n",
    "\n",
    "**Block pointers** are Triton's high-level abstraction for tiled memory access, eliminating manual pointer arithmetic.\n",
    "\n",
    "**Core concept:** Instead of computing `ptr + offset`, block pointers encapsulate:\n",
    "- Where you are in the tensor (offsets)\n",
    "- What tile you're accessing (block_shape)\n",
    "- How to navigate memory (strides)\n",
    "\n",
    "---\n",
    "\n",
    "**Breaking down `x_block_ptr` (2D tensor):**\n",
    "```python\n",
    "x_block_ptr = tl.make_block_ptr(\n",
    "    x_ptr,                                    # Base address\n",
    "    shape=(ROWS, D),                          # Full tensor: ROWS √ó D\n",
    "    strides=(x_stride_row, x_stride_dim),     # Jump between rows/cols\n",
    "    offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),  # Start at row tile, col 0\n",
    "    block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),   # Process ROWS_TILE_SIZE √ó D_TILE_SIZE tile\n",
    "    order=(1, 0),                             # Row-major (dim 1 contiguous)\n",
    ")\n",
    "```\n",
    "\n",
    "**Visual:**\n",
    "```\n",
    "Full tensor (ROWS √ó D):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ [0,0]  ......  [0, D-1]         ‚îÇ ‚Üê row_tile_idx=0, loads ROWS_TILE_SIZE rows\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ [ROWS_TILE_SIZE, 0] ...         ‚îÇ ‚Üê row_tile_idx=1\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ...                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îî‚îÄ D_TILE_SIZE ‚îÄ‚îò  (tile width)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Breaking down `weight_block_ptr` (1D tensor):**\n",
    "```python\n",
    "weight_block_ptr = tl.make_block_ptr(\n",
    "    weight_ptr,\n",
    "    shape=(D,),                    # 1D vector of length D\n",
    "    strides=(weight_stride_dim,),  # Jump between elements\n",
    "    offsets=(0,),                  # Start at index 0 (no row tiling for weight)\n",
    "    block_shape=(D_TILE_SIZE,),    # Load D_TILE_SIZE elements at a time\n",
    "    order=(0,),                    # 1D ordering\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Breaking down `output_block_ptr` (1D tensor):**\n",
    "```python\n",
    "output_block_ptr = tl.make_block_ptr(\n",
    "    output_ptr,\n",
    "    shape=(ROWS,),                           # 1D vector of length ROWS\n",
    "    strides=(output_stride_row,),            # Jump between elements\n",
    "    offsets=(row_tile_idx * ROWS_TILE_SIZE,), # Start at current row tile\n",
    "    block_shape=(ROWS_TILE_SIZE,),           # Write ROWS_TILE_SIZE elements\n",
    "    order=(0,),\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Usage in weighted sum kernel:**\n",
    "```python\n",
    "# Initialize accumulator\n",
    "output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "\n",
    "# Loop over D dimension in tiles\n",
    "for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "    # Load current tiles with boundary checking\n",
    "    # boundary_check=(0, 1) checks both dimensions (rows and cols)\n",
    "    row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "    # Shape: (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "    \n",
    "    weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
    "    # Shape: (D_TILE_SIZE,)\n",
    "    \n",
    "    # Compute weighted sum: sum over columns for each row\n",
    "    output += tl.sum(row * weight[None, :], axis=1)  # Accumulate partial sums\n",
    "    \n",
    "    # Advance to next tile along D dimension\n",
    "    x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))      # Move right\n",
    "    weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))  # Move forward\n",
    "\n",
    "# Write final result\n",
    "tl.store(output_block_ptr, output, boundary_check=(0,))\n",
    "```\n",
    "\n",
    "**Key features:**\n",
    "\n",
    "1. **Boundary checking:** `boundary_check=(0, 1)` ensures safe access when tile doesn't fit perfectly\n",
    "   - Dimension 0 (rows): May not divide evenly by ROWS_TILE_SIZE\n",
    "   - Dimension 1 (cols): May not divide evenly by D_TILE_SIZE\n",
    "\n",
    "2. **Tiled computation:** Loop processes D dimension in chunks of D_TILE_SIZE, accumulating results\n",
    "\n",
    "3. **Advancing pointers:** `.advance()` moves to next tile without manual offset computation\n",
    "\n",
    "**vs. Manual pointer arithmetic:**\n",
    "```python\n",
    "# OLD WAY (error-prone):\n",
    "row_offsets = row_tile_idx * ROWS_TILE_SIZE + tl.arange(0, ROWS_TILE_SIZE)\n",
    "col_offsets = tl.arange(0, D_TILE_SIZE)\n",
    "x_ptrs = x_ptr + row_offsets[:, None] * x_stride_row + col_offsets[None, :] * x_stride_dim\n",
    "row = tl.load(x_ptrs, mask=(row_offsets < ROWS)[:, None] & (col_offsets < D)[None, :])\n",
    "\n",
    "# NEW WAY (cleaner):\n",
    "x_block_ptr = tl.make_block_ptr(...)\n",
    "row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Automatic bounds checking via `boundary_check`\n",
    "- Cleaner code with `.advance()` instead of manual offset arithmetic\n",
    "- Better optimization opportunities for compiler\n",
    "- Handles non-contiguous tensors correctly via strides\n",
    "\n",
    "The code below is taken from the [Stanford course cs336](https://github.com/stanford-cs336/assignment2-systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c03191b-645c-4c91-9423-f5ea74b3137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,  # Input pointers\n",
    "    output_ptr,  # Output pointer\n",
    "    x_stride_row, x_stride_dim,  # Strides tell us how to move one element in each axis of a tensor\n",
    "    weight_stride_dim,  # Likely 1\n",
    "    output_stride_row,  # Likely 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # Tile shapes must be known at compile time\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a tile of rows of x.\n",
    "    # `tl.program_id` gives us a way to check which thread block we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    \n",
    "    # Block pointers give us a way to select from an ND region of memory\n",
    "    # and move our selection around.\n",
    "    # The block pointer must know:\n",
    "    # - The pointer to the first element of the tensor\n",
    "    # - The overall shape of the tensor to handle out-of-bounds access\n",
    "    # - The strides of each dimension to use the memory layout properly\n",
    "    # - The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "    # - The block shape to use load/store at a time\n",
    "    # - The order of the dimensions in memory from major to minor\n",
    "    # axes (= np.argsort(strides)) for optimizations, especially useful on H100\n",
    "    \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    # Initialize a buffer to write to\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "    \n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        # Load the current block pointer\n",
    "        # Since ROWS_TILE_SIZE might not divide ROWS, and D_TILE_SIZE might not divide D,\n",
    "        # we need boundary checks for both dimensions\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "        \n",
    "        # Compute the weighted sum of the row.\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "        \n",
    "        # Move the pointers to the next tile.\n",
    "        # These are (rows, columns) coordinate deltas\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # Move by D_TILE_SIZE in the last dimension\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # Move by D_TILE_SIZE\n",
    "    \n",
    "    # Write output to the output block pointer (a single scalar per row).\n",
    "    # Since ROWS_TILE_SIZE might not divide ROWS, we need boundary checks\n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1707e502-0d50-4ecb-94fb-bd5850e078e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_triton(x: torch.Tensor, weight: torch.Tensor):\n",
    "    D = x.shape[-1]\n",
    "    output_dims = x.shape[:-1]   \n",
    "    # Reshape input tensor to 2D\n",
    "    \n",
    "    x = rearrange(x, \"... d -> (...) d\")\n",
    "    # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "    y = torch.empty(x.shape[0], device=x.device)\n",
    "\n",
    "    D_TILE_SIZE = triton.next_power_of_2(D) // 16  # Roughly 16 loops through the embedding dimension\n",
    "    ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "        \n",
    "    # Launch our kernel with n instances in our 1D grid.\n",
    "    n_rows = y.numel()\n",
    "    weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "        \n",
    "    return y.view(output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bd16f2a-719a-4bfe-a254-5c40ee7bd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal3(f1, f2):\n",
    "    x = torch.randn(64, 64, 2048, device=get_device())\n",
    "    w = torch.randn(2048, device=get_device())\n",
    "    y1 = f1(x,w)\n",
    "    y2 = f2(x,w)\n",
    "    assert torch.allclose(y1, y2, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c361f23-92f7-4543-90f7-07be6e6bdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_equal3(weighted_sum,weighted_sum_triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3769993-6ad7-45fe-8e4b-df074b8708a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # Cache x and weight to be used in the backward pass, when we\n",
    "        # only receive the gradient wrt. the output tensor, and\n",
    "        # need to compute the gradients wrt. x and weight.\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "\n",
    "        # Reshape input tensor to 2D\n",
    "        x_reshaped = rearrange(x, \"... d -> (...) d\")\n",
    "        ctx.output_dims = output_dims\n",
    "\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
    "        assert (\n",
    "            x_reshaped.is_contiguous()\n",
    "        ), \"Our pointer arithmetic will assume contiguous x\"\n",
    "\n",
    "        D_TILE_SIZE = (\n",
    "            triton.next_power_of_2(D) // 16\n",
    "        )  # Roughly 16 loops through the embedding dimension\n",
    "        ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "\n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "        y = torch.empty(x_reshaped.shape[0], device=x.device)\n",
    "\n",
    "        # Launch our kernel with n instances in our 1D grid.\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x_reshaped,\n",
    "            weight,\n",
    "            y,\n",
    "            x_reshaped.stride(0),\n",
    "            x_reshaped.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows,\n",
    "            D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE,\n",
    "            D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        return y.view(output_dims)\n",
    "\n",
    "    # Here you should make a triton kernel for the backward instead of plain PyTorch!\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved tensors\n",
    "        x, weight = ctx.saved_tensors\n",
    "\n",
    "        # Reshape grad_output to match forward pass\n",
    "        grad_output_flat = grad_output.reshape(-1)\n",
    "\n",
    "        # Gradient wrt weight: sum over all samples\n",
    "        # d/dw (w^T x) = x\n",
    "        # So grad_weight = sum_i grad_output[i] * x[i]\n",
    "        grad_weight = (grad_output_flat[:, None] * x).sum(dim=0)\n",
    "\n",
    "        # Gradient wrt x: broadcast weight\n",
    "        # d/dx (w^T x) = w\n",
    "        # So grad_x = grad_output * w\n",
    "        grad_x = grad_output_flat[:, None] * weight[None, :]\n",
    "\n",
    "        # Reshape grad_x back to original shape\n",
    "        grad_x = grad_x.view(*ctx.output_dims, -1)\n",
    "\n",
    "        return grad_x, grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea0b14bf-2733-4732-9d7a-2c1795b20891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionTriton(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear regression using the custom Triton weighted sum kernel.\n",
    "\n",
    "    Model: y = w^T x + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(input_dim, device=\"cuda\") * 0.01)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1, device=\"cuda\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        # Use our custom weighted sum function\n",
    "        return WeightedSumFunc.apply(x, self.weight) + self.bias\n",
    "\n",
    "\n",
    "def generate_regression_data(n_samples=1000, input_dim=128, noise_std=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic linear regression data.\n",
    "\n",
    "    Returns:\n",
    "        X: (n_samples, input_dim) feature matrix\n",
    "        y: (n_samples,) continuous target values\n",
    "        true_weight: (input_dim,) true weight vector used for generation\n",
    "        true_bias: (1,) true bias value used for generation\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate random features\n",
    "    X = torch.randn(n_samples, input_dim, device=\"cuda\")\n",
    "\n",
    "    # Create true weights for data generation\n",
    "    true_weight = torch.randn(input_dim, device=\"cuda\")\n",
    "    true_bias = torch.randn(1, device=\"cuda\")\n",
    "\n",
    "    # Generate target values: y = w^T x + b + noise\n",
    "    y = X @ true_weight + true_bias\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    noise = torch.randn(n_samples, device=\"cuda\") * noise_std\n",
    "    y = y + noise\n",
    "\n",
    "    return X, y, true_weight, true_bias\n",
    "\n",
    "\n",
    "def train_linear_regression(\n",
    "    model, X_train, y_train, X_val, y_val, epochs=100, lr=0.01, batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the linear regression model.\n",
    "\n",
    "    Args:\n",
    "        model: LinearRegressionTriton model\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        batch_size: Batch size for training\n",
    "\n",
    "    Returns:\n",
    "        train_losses: List of training losses (MSE) per epoch\n",
    "        val_losses: List of validation losses (MSE) per epoch\n",
    "        val_r2_scores: List of validation R¬≤ scores per epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_r2_scores = []\n",
    "\n",
    "    n_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(len(X_train), device=\"cuda\")\n",
    "        X_train_shuffled = X_train[perm]\n",
    "        y_train_shuffled = y_train[perm]\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(X_train))\n",
    "\n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Calculate R¬≤ score\n",
    "            ss_res = ((y_val - y_val_pred) ** 2).sum()\n",
    "            ss_tot = ((y_val - y_val.mean()) ** 2).sum()\n",
    "            r2_score = 1 - (ss_res / ss_tot)\n",
    "            val_r2_scores.append(r2_score.item())\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                f\"Train Loss = {avg_train_loss:.4f}, \"\n",
    "                f\"Val Loss = {val_loss:.4f}, \"\n",
    "                f\"Val R¬≤ = {r2_score.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    return train_losses, val_losses, val_r2_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cce9df3c-9b9d-48d2-a6b7-37ddd2d142ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate linear regression with custom Triton kernel.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Linear Regression with Custom Triton Weighted Sum Kernel\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Check CUDA availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ERROR: CUDA is not available. This example requires a GPU.\")\n",
    "        return\n",
    "\n",
    "    # Hyperparameters\n",
    "    n_train = 800\n",
    "    n_val = 200\n",
    "    input_dim = 128\n",
    "    epochs = 100\n",
    "    lr = 0.01\n",
    "    batch_size = 64\n",
    "    noise_std = 0.1\n",
    "\n",
    "    print(f\"\\nDataset configuration:\")\n",
    "    print(f\"  Training samples: {n_train}\")\n",
    "    print(f\"  Validation samples: {n_val}\")\n",
    "    print(f\"  Input dimension: {input_dim}\")\n",
    "    print(f\"  Noise std: {noise_std}\")\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Learning rate: {lr}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print()\n",
    "\n",
    "    # Generate data\n",
    "    print(\"Generating synthetic linear regression data...\")\n",
    "    X, y, true_weight, true_bias = generate_regression_data(\n",
    "        n_samples=n_train + n_val, input_dim=input_dim, noise_std=noise_std\n",
    "    )\n",
    "\n",
    "    # Split into train and validation\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:], y[n_train:]\n",
    "\n",
    "    print(f\"Training set: X shape = {X_train.shape}, y shape = {y_train.shape}\")\n",
    "    print(f\"Validation set: X shape = {X_val.shape}, y shape = {y_val.shape}\")\n",
    "    print(f\"\\nTrue parameters:\")\n",
    "    print(f\"  True weight norm: {true_weight.norm().item():.6f}\")\n",
    "    print(f\"  True bias: {true_bias.item():.6f}\")\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing linear regression model with Triton kernel...\")\n",
    "    model = LinearRegressionTriton(input_dim=input_dim)\n",
    "    model = torch.compile(model)\n",
    "    print(\n",
    "        f\"Model parameters: weight shape = {model.weight.shape}, bias shape = {model.bias.shape}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\\n\")\n",
    "    train_losses, val_losses, val_r2_scores = train_linear_regression(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Final Training Loss (MSE): {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss (MSE): {val_losses[-1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Compare learned parameters with true parameters\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Parameter Comparison: Learned vs True\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    learned_weight = model.weight.data\n",
    "    learned_bias = model.bias.data\n",
    "\n",
    "    # Compute various comparison metrics\n",
    "    weight_diff = learned_weight - true_weight\n",
    "    weight_mse = (weight_diff**2).mean().item()\n",
    "    weight_mae = weight_diff.abs().mean().item()\n",
    "    \n",
    "\n",
    "    bias_diff = (learned_bias - true_bias).abs().item()\n",
    "\n",
    "\n",
    "    print(f\"\\nWeight Statistics:\")\n",
    "    print(f\"  True weight norm:        {true_weight.norm().item():.6f}\")\n",
    "    print(f\"  Learned weight norm:     {learned_weight.norm().item():.6f}\")\n",
    "    print(f\"  Weight MSE:              {weight_mse:.6f}\")\n",
    "    print(f\"  Weight MAE:              {weight_mae:.6f}\")\n",
    "    \n",
    "    print(f\"\\nBias Statistics:\")\n",
    "    print(f\"  True bias:               {true_bias.item():.6f}\")\n",
    "    print(f\"  Learned bias:            {learned_bias.item():.6f}\")\n",
    "    print(f\"  Bias absolute difference: {bias_diff:.6f}\")\n",
    "    print()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Example completed successfully!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e789074-7bd9-4ec6-8c31-eacda7522441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Linear Regression with Custom Triton Weighted Sum Kernel\n",
      "================================================================================\n",
      "\n",
      "Dataset configuration:\n",
      "  Training samples: 800\n",
      "  Validation samples: 200\n",
      "  Input dimension: 128\n",
      "  Noise std: 0.1\n",
      "\n",
      "Training configuration:\n",
      "  Epochs: 100\n",
      "  Learning rate: 0.01\n",
      "  Batch size: 64\n",
      "\n",
      "Generating synthetic linear regression data...\n",
      "Training set: X shape = torch.Size([800, 128]), y shape = torch.Size([800])\n",
      "Validation set: X shape = torch.Size([200, 128]), y shape = torch.Size([200])\n",
      "\n",
      "True parameters:\n",
      "  True weight norm: 10.257113\n",
      "  True bias: -0.518749\n",
      "\n",
      "Initializing linear regression model with Triton kernel...\n",
      "Model parameters: weight shape = torch.Size([128]), bias shape = torch.Size([1])\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/100: Train Loss = 83.6631, Val Loss = 65.1403, Val R¬≤ = 0.3941\n",
      "Epoch 10/100: Train Loss = 1.4423, Val Loss = 2.0460, Val R¬≤ = 0.9810\n",
      "Epoch 20/100: Train Loss = 0.0760, Val Loss = 0.1423, Val R¬≤ = 0.9987\n",
      "Epoch 30/100: Train Loss = 0.0143, Val Loss = 0.0264, Val R¬≤ = 0.9998\n",
      "Epoch 40/100: Train Loss = 0.0091, Val Loss = 0.0142, Val R¬≤ = 0.9999\n",
      "Epoch 50/100: Train Loss = 0.0087, Val Loss = 0.0128, Val R¬≤ = 0.9999\n",
      "Epoch 60/100: Train Loss = 0.0087, Val Loss = 0.0125, Val R¬≤ = 0.9999\n",
      "Epoch 70/100: Train Loss = 0.0087, Val Loss = 0.0126, Val R¬≤ = 0.9999\n",
      "Epoch 80/100: Train Loss = 0.0090, Val Loss = 0.0126, Val R¬≤ = 0.9999\n",
      "Epoch 90/100: Train Loss = 0.0088, Val Loss = 0.0125, Val R¬≤ = 0.9999\n",
      "Epoch 100/100: Train Loss = 0.0087, Val Loss = 0.0126, Val R¬≤ = 0.9999\n",
      "\n",
      "================================================================================\n",
      "Training Complete!\n",
      "================================================================================\n",
      "Final Training Loss (MSE): 0.0087\n",
      "Final Validation Loss (MSE): 0.0126\n",
      "\n",
      "================================================================================\n",
      "Parameter Comparison: Learned vs True\n",
      "================================================================================\n",
      "\n",
      "Weight Statistics:\n",
      "  True weight norm:        10.257113\n",
      "  Learned weight norm:     10.255894\n",
      "  Weight MSE:              0.000015\n",
      "  Weight MAE:              0.003088\n",
      "\n",
      "Bias Statistics:\n",
      "  True bias:               -0.518749\n",
      "  Learned bias:            -0.512794\n",
      "  Bias absolute difference: 0.005954\n",
      "\n",
      "================================================================================\n",
      "Example completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc87617-83f1-4839-880d-e5aba96500ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-syst",
   "language": "python",
   "name": "cs336-syst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
