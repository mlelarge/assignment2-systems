{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57624bc4-fb33-40cf-bdf3-5aa25f89d22f",
   "metadata": {},
   "source": [
    "# Programming on GPUs\n",
    "\n",
    "The goal of this notebook is to learn the basics of programming on GPUs. We start with [Numba](https://numba.pydata.org/) a just-in-time compiler for Python and then move on to [Triton](https://openai.com/index/triton/) an open-source Python-like programming language developed by OpenAI.\n",
    "\n",
    "You will probably need to learn a lot of new concepts but this notebook will ask you to code in an interactive way with minimal presentation of the core concepts. If you are stuck ask for help to your favorite chat without asking for the exact solution.\n",
    "\n",
    "Sources: Nvidia [CUDA documentation](https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html) and [GPU-Puzzles](https://github.com/srush/GPU-Puzzles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e256bcb-6387-4811-9959-f448426b42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cd6e1d-a546-4cef-b9a4-d0f42e24ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", category=numba.NumbaPerformanceWarning, module=\"numba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929cee2-ac79-4576-9c9c-4442ef7123ff",
   "metadata": {},
   "source": [
    "## Main Concepts\n",
    "\n",
    "- **Streaming Multiprocessor (SM)** GPU's unit of computation \n",
    "- **Thread** smallest unit of execution\n",
    "- **Block or Thread Block** a group of threads guaranteed to run on a single SM \n",
    "- **Grid** Thread blocks are organized into a grid\n",
    "- **Warp** Within a thread block, threads are organized into groups of 32 threads called warps. A warp executes the kernel code in a Single-Instruction Multiple-Threads (SIMT) paradigm. In SIMT, all threads in the warp are executing the same kernel code.\n",
    "\n",
    "Let's start! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2911024-cc0b-46e9-9685-e1e6710f5920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 11., 12., 13.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_spec(a):\n",
    "    return a + 10\n",
    "\n",
    "# Size of our array\n",
    "SIZE = 4\n",
    "\n",
    "# Create input and output arrays\n",
    "a = np.arange(SIZE, dtype=np.float32)  # [0, 1, 2, 3]\n",
    "out = np.zeros(SIZE, dtype=np.float32)\n",
    "\n",
    "map_spec(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7924d20-47ab-4310-98fe-134781b5f651",
   "metadata": {},
   "source": [
    "Now we want to code this so that each thread adds 10 to one element of the array using Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a463ab-87e8-4a84-9c59-793297551b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    [0. 1. 2. 3.]\n",
      "Output:   [10. 11. 12. 13.]\n",
      "Expected: [10. 11. 12. 13.]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def map_kernel(out, a):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # Each thread adds 10 to one element\n",
    "    # your code here\n",
    "    out[i] = a[i] + 10\n",
    "\n",
    "\n",
    "# Copy arrays to GPU\n",
    "a_device = cuda.to_device(a)\n",
    "out_device = cuda.to_device(out)\n",
    "\n",
    "# kernel[grid, block](args)\n",
    "# Launch kernel: grid = 1 block, block = SIZE threads\n",
    "map_kernel[1, SIZE](out_device, a_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Input:    {a}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e2d63-6175-4c4b-95c2-c425a0804ce7",
   "metadata": {},
   "source": [
    "Let's move to vector addition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b8fcdf-cd27-4bb3-becd-60661a021407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zip_spec(a, b):\n",
    "    return a + b\n",
    "\n",
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "zip_spec(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90295c2d-1514-46f5-a755-10cd76e7e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a:  [0 1 2 3]\n",
      "Input b:  [0 1 2 3]\n",
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def zip_kernel(out, a, b):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # your code here\n",
    "    out[i] = a[i] + b[i]\n",
    "\n",
    "# A function to move vectors on device\n",
    "def init_pb(a=a, b=b, out=out):\n",
    "    a_device = cuda.to_device(a)\n",
    "    b_device = cuda.to_device(b)\n",
    "    out_device = cuda.to_device(out)\n",
    "    return a_device, b_device, out_device\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "# Launch kernel: 1 block, SIZE threads\n",
    "zip_kernel[1, SIZE](out_device, a_device, b_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Input a:  {a}\")\n",
    "print(f\"Input b:  {b}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b00ec4-6aa5-44e8-8a79-0e9bb0a6ac67",
   "metadata": {},
   "source": [
    "What happens if you increase the number of threads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ed1ebc-96fd-4d7c-afc2-172f47b48840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "NUM_TRHEADS = 2*SIZE\n",
    "zip_kernel[1, NUM_TRHEADS](out_device, a_device, b_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b977472-73cf-49cf-a808-1b79ffce48d5",
   "metadata": {},
   "source": [
    "Still working but unsafe (due to out-of-bounds access). Add a guard clause to prevent out-of-bounds access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47d9b2f-d542-4286-aa78-19152ce85474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# CUDA kernel with Guard\n",
    "@cuda.jit\n",
    "def zip_guard_kernel(out, a, b, size):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # your code here\n",
    "    if i < size:\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "NUM_TRHEADS = 2*SIZE\n",
    "zip_guard_kernel[1, NUM_TRHEADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb57cf-877f-4c2b-839a-ebf1f5ef877f",
   "metadata": {},
   "source": [
    "Let's move to matrices now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35e349e-0d40-43a0-b409-79ad7836d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 11, 12, 13],\n",
       "       [14, 15, 16, 17],\n",
       "       [18, 19, 20, 21],\n",
       "       [22, 23, 24, 25]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))\n",
    "out = map_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40147ee-56f6-4479-9eb1-58e8327fa17c",
   "metadata": {},
   "source": [
    "So far, we used 1-D **Thread Block** but **Thread Blocks** can be organized in 2-D or 3-D. These dimensions can simplify mapping of individual threads to units of work or data items.\n",
    "Below, you need to use a 2-D **Thread Block**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948815aa-6a9e-428d-b472-f9858ef1dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[10 11 12 13]\n",
      " [14 15 16 17]\n",
      " [18 19 20 21]\n",
      " [22 23 24 25]]\n",
      "Expected: [[10 11 12 13]\n",
      " [14 15 16 17]\n",
      " [18 19 20 21]\n",
      " [22 23 24 25]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def map_2d_kernel(out, a, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    # your code here\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,j] + 10\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "TRHEAD_BLOCK = (SIZE, SIZE)\n",
    "map_2d_kernel[1, TRHEAD_BLOCK](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9525d76-f77e-4d89-9a36-9081e2f4f135",
   "metadata": {},
   "source": [
    "Broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8bc22dc-5282-4687-a058-c14441863971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [1, 2, 3, 4],\n",
       "       [2, 3, 4, 5],\n",
       "       [3, 4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(SIZE).reshape(SIZE, 1)\n",
    "b = np.arange(SIZE).reshape(1, SIZE)\n",
    "out = a + b\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858dcc2a-20ed-472c-8125-7621ea5c8c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_kernel(out, a, b, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    #your code here\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREAD_BLOCK = (2*SIZE, 3*SIZE)\n",
    "broadcast_kernel[1, THREAD_BLOCK](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc96aa9-bacd-4355-b86f-47d58c779687",
   "metadata": {},
   "source": [
    "So far, we did not use **grid** but like **Thread Block**, grids may be 1, 2, or 3 dimensional. Below use a 2D grid with a single thread per block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed23d449-fcd6-44d1-a150-ea58779f1798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x \n",
    "    j = cuda.blockIdx.y\n",
    "    # your code here\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "# 1 thread per block, 2D grid\n",
    "THREADS = 1\n",
    "GRID = (SIZE, SIZE)\n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879131b7-cabf-47bc-ae67-0eedb3e1cba2",
   "metadata": {},
   "source": [
    "Below, you need to use a 1D grid with a 1D Thread block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2281d6cd-4cc4-45f9-859c-e3dda4586a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [2 0 0 0]\n",
      " [3 0 0 0]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  False\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    # your code here\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE\n",
    "GRID = SIZE\n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71de175-064f-4db6-9f9b-2ec9e397894d",
   "metadata": {},
   "source": [
    "And now, use small 2D grid and blocks.\n",
    "Here we have `cuda.blockDim.x = cuda.blockDim.y =2 ` and sinc `SIZE//2 = 2`, you need to compute i,j as below:\n",
    "\n",
    "| blockIdx.x | blockIdx.y  | threadIdx.x | threadIdx.y | **i** | **j** | Computes |\n",
    "|------------|---------------|-------------|-------------|-------|-------|----------|\n",
    "| 0 | 0 | 0 | 0 | **0** | **0** | out[0,0] |\n",
    "| 0 | 0 | 1 | 0 | **1** | **0** | out[1,0] |\n",
    "| 0 | 0 | 0 | 1 | **0** | **1** | out[0,1] |\n",
    "| 0 | 0 | 1 | 1 | **1** | **1** | out[1,1] |\n",
    "| 1 | 0 | 0 | 0 | **2** | **0** | out[2,0] |\n",
    "| 1 | 0 | 1 | 0 | **3** | **0** | out[3,0] |\n",
    "| 0 | 1 | 0 | 0 | **0** | **2** | out[0,2] |\n",
    "| 0 | 1 | 1 | 1 | **1** | **3** | out[1,3] |\n",
    "| 1 | 1 | 0 | 0 | **2** | **2** | out[2,2] |\n",
    "| 1 | 1 | 1 | 1 | **3** | **3** | out[3,3] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea2afc6-c4c4-4276-a79e-190ade51d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    # your code here\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = (SIZE//2 , SIZE//2)\n",
    "GRID = (SIZE//2, SIZE//2)  \n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e0e2a-602b-4d0b-b4b0-abef682330dc",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "<div>\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-programming-guide/_images/gpu-cpu-system-diagram.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "- **Dynamic random-access memory DRAM** DRAM attached to the GPU is called **global memory**, because it is accessible to all SMs in the GPU. The DRAM attached to the CPU(s) is called system memory or host memory.\n",
    "- On-Chip Memory in GPUs: **Shared Memory** and **Registers**. Programmers use this shared memory as a manually managed cache. Threads have access to private **registers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7633ae72-85e7-4196-bde1-fd187b750b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  3.,  6.,  9., 12., 15., 18.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool_spec(a):\n",
    "    out = np.zeros(a.shape)\n",
    "    for i in range(a.shape[0]):\n",
    "        out[i] = a[max(i - 2, 0) : i + 1].sum()\n",
    "    return out\n",
    "\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE)\n",
    "out = pool_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac15df9-9dcd-4493-bf31-571f774801a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Expected: [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Correct:  True\n",
      "number of access to global memory: 1 + 2 + 2 threads x 3 reads = 9 global reads per block -> 18 global reads in total\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def pool_kernel(out, a, size):\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        # Manually compute sum - can't use slicing in CUDA!\n",
    "        temp_sum = 0.0\n",
    "        for k in range(max(i - 2, 0), i + 1):\n",
    "            temp_sum += a[k]  # Use global memory\n",
    "        out[i] = temp_sum\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE//2\n",
    "GRID = (2,1)  \n",
    "pool_kernel[GRID, THREADS](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")\n",
    "r = 3\n",
    "print(f\"number of access to global memory: 1 + 2 + {THREADS-2} threads x {r} reads = {1+2+(THREADS-2)*r} global reads per block -> {2*(1+2+(THREADS-2)*r)} global reads in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9017cc-7569-48d5-89e0-fc931f89126a",
   "metadata": {},
   "source": [
    "So now, we want to get a lower number of access to global memory by using shared memory.\n",
    "\n",
    "```\n",
    "Memory hierarchy:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Global Memory (a, out)             ‚îÇ  ‚Üê ALL threads can access\n",
    "‚îÇ  - Slow                             ‚îÇ\n",
    "‚îÇ  - Accessible across all blocks     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì                    ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ Block 0 ‚îÇ          ‚îÇ Block 1 ‚îÇ\n",
    "   ‚îÇ Shared  ‚îÇ          ‚îÇ Shared  ‚îÇ      ‚Üê Only threads in THIS block\n",
    "   ‚îÇ Memory  ‚îÇ          ‚îÇ Memory  ‚îÇ         can access\n",
    "   ‚îÇ (fast)  ‚îÇ          ‚îÇ (fast)  ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "The method below allows for 8+4=12 global reads in total.\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Global:      [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "Block 0 loads:              Block 1 loads:\n",
    "        ‚Üì                          ‚Üì\n",
    "Shared: [0, 0, 0, 1, 2, 3] Shared: [2, 3, 4, 5, 6, 7]\n",
    "         ‚îî‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        halo    main data          halo    main data\n",
    "```\n",
    "\n",
    "Each block can only have a constant amount of shared memory that threads in that block can read and write to. This needs to be a literal python constant not a variable. After writing to shared memory you need to call `cuda.syncthreads` to ensure that threads do not cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92a76dde-423b-459d-8027-4eb99c3a0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Expected: [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "TPB = 4  # Threads per block\n",
    "SharedMem = TPB + 2 # cannot be computed at runtime\n",
    "@cuda.jit\n",
    "def pool_kernel_shared(out, a, size):\n",
    "    # Allocate shared memory with HALO (extra elements for boundary)\n",
    "    # Need TPB + 2 extra elements (for the 2-element lookback)\n",
    "    shared = cuda.shared.array(SharedMem, numba.float32)\n",
    "    \n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    local_i = cuda.threadIdx.x\n",
    "    \n",
    "    # Each thread loads its own element into shared memory (offset by 2 for halo)\n",
    "    if i < size:\n",
    "        shared[local_i + 2] = a[i]\n",
    "    \n",
    "    # First 2 threads also load the HALO (left boundary elements)\n",
    "    if local_i == 0:\n",
    "        # Load 2 elements before the block starts\n",
    "        start_idx = cuda.blockIdx.x * cuda.blockDim.x\n",
    "        if start_idx >= 2:\n",
    "            shared[1] = a[start_idx - 1]\n",
    "            shared[0] = a[start_idx - 2]\n",
    "        else:\n",
    "            shared[1] = 0.0\n",
    "            shared[0] = 0.0 # Padding for out-of-bounds\n",
    "            \n",
    "    # Wait for all threads to finish loading\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Now compute using shared memory\n",
    "    if i < size:\n",
    "        temp_sum = 0.0\n",
    "        # Look back up to 2 elements in shared memory\n",
    "        for k in range(max(0, 3 - (i + 1)), 3):  # At most 3 elements\n",
    "            temp_sum += shared[local_i + 2 - (2 - k)]\n",
    "        out[i] = temp_sum\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "\n",
    "GRID = (SIZE // TPB, 1)  # (2, 1) for SIZE=8, TPB=4\n",
    "pool_kernel_shared[GRID, TPB](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad89325-3bfe-4bd0-8905-ef6b43beccb8",
   "metadata": {},
   "source": [
    "Now, we need to implement the dot product. Serially, the code is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce013ae9-bb0e-42e5-a36e-2f3e30c28da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(140.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_spec(a,b):\n",
    "    tot = 0\n",
    "    for i in range(len(a)):\n",
    "        tot += a[i]*b[i]\n",
    "    return tot\n",
    "\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "dot_spec(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c68f3-7961-4ed5-ba8a-db7ea7a5e8a2",
   "metadata": {},
   "source": [
    "In parallel, the problem is a lot harder. Here is a a tree-based summation algorithm:\n",
    "\n",
    "![](https://www.cs.uaf.edu/2012/fall/cs441/lecture/tree_sum_16td.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d852a2b3-62a3-4bbb-b0e8-98d0900447bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(108611072.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_tree(a,b):\n",
    "    size = len(a)\n",
    "    shared_mem = np.zeros(size)\n",
    "    for i in range(size):\n",
    "       shared_mem[i] = a[i]*b[i]\n",
    "    stride = size // 2\n",
    "    while stride > 0:\n",
    "        for i in range(stride):\n",
    "            shared_mem[i] += shared_mem[i+stride]\n",
    "        stride //=2\n",
    "    return shared_mem[0]\n",
    "dot_tree(a,b)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab60ed8-d6d8-4fb8-ab1d-eb680df8a981",
   "metadata": {},
   "source": [
    "Now you need to code this algorithm in numba. You have 256 threads per block and sufficient blocks to have more threads than the size of your input vectors `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8a757d9-7dac-44ae-a18d-ab3a01090926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threads per block: 256\n",
      "blocks per grid: 4\n"
     ]
    }
   ],
   "source": [
    "SIZE = 800\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (SIZE + threads_per_block - 1) // threads_per_block\n",
    "print(f\"threads per block: {threads_per_block}\")\n",
    "print(f\"blocks per grid: {blocks_per_grid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903affae-6e73-441f-a126-5fa6d42fa1d6",
   "metadata": {},
   "source": [
    "For each block, you can implement the tree-based summation algorithm by first creating a shared memory of size 256 containing the `a[i]*b[i]` with the `i` associated witht the block thread. Then suming it accross the block. The last step consists in adding all the intermediate results: each block adds its result. For this last step, you might want to use `cuda.atomic.add` see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaffa23-43cd-4bcf-aa9d-d291dc1f0df5",
   "metadata": {},
   "source": [
    "### The Problem Without Atomics\n",
    "When you write out[0] = out[0] + value, it's actually 3 separate steps:\n",
    "```\n",
    "# out[0] = out[0] + value breaks down to:\n",
    "1. READ:   temp = out[0]      # Read current value\n",
    "2. MODIFY: temp = temp + value # Add to it\n",
    "3. WRITE:  out[0] = temp       # Write back\n",
    "```\n",
    "\n",
    "**With multiple threads, these steps can interleave and lose updates:**\n",
    "```\n",
    "Initial: out[0] = 0\n",
    "\n",
    "Thread A (Block 0):              Thread B (Block 1):\n",
    "1. READ: temp_A = 0             \n",
    "2. MODIFY: temp_A = 0 + 5       \n",
    "                                 1. READ: temp_B = 0      ‚Üê Still sees 0!\n",
    "3. WRITE: out[0] = 5            \n",
    "                                 2. MODIFY: temp_B = 0 + 3 ‚Üê Uses old value!\n",
    "                                 3. WRITE: out[0] = 3      ‚Üê Overwrites 5!\n",
    "\n",
    "Final: out[0] = 3  ‚ùå Should be 8!\n",
    "```\n",
    "\n",
    "### What Atomics Do\n",
    "\n",
    "`cuda.atomic.add(out, 0, value)` **locks the memory location** so the entire operation completes before another thread can access it:\n",
    "```\n",
    "Initial: out[0] = 0\n",
    "\n",
    "Thread A (Block 0):              Thread B (Block 1):\n",
    "üîí LOCK out[0]\n",
    "1. READ: temp_A = 0             \n",
    "2. MODIFY: temp_A = 0 + 5       \n",
    "3. WRITE: out[0] = 5            \n",
    "üîì UNLOCK out[0]\n",
    "                                 üîí LOCK out[0]  ‚Üê Must wait for unlock\n",
    "                                 1. READ: temp_B = 5      ‚Üê Sees updated value!\n",
    "                                 2. MODIFY: temp_B = 5 + 3\n",
    "                                 3. WRITE: out[0] = 8\n",
    "                                 üîì UNLOCK out[0]\n",
    "\n",
    "Final: out[0] = 8  ‚úÖ Correct!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a6919a7-95f1-4887-b86d-667784b92405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA result: 170346800.0\n",
      "NumPy result: 170346800.0\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def dot_kernel_numba(a, b, out, size):\n",
    "    shared = cuda.shared.array(256, numba.float32)\n",
    "    # your code here\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    local_i = cuda.threadIdx.x\n",
    "    \n",
    "    if i < size:\n",
    "        shared[local_i] = a[i] * b[i]\n",
    "    else:\n",
    "        shared[local_i] = 0.0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if local_i < stride:\n",
    "            shared[local_i] += shared[local_i + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "    \n",
    "    if local_i == 0:\n",
    "        cuda.atomic.add(out, 0, shared[0]) # To avoid RACE CONDITION! Multiple blocks write to out[0] see below\n",
    "\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like([expected]))\n",
    "\n",
    "size = a_device.shape[0]\n",
    "dot_kernel_numba[blocks_per_grid, threads_per_block](a_device, b_device, out_device, size)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "expected = np.dot(a, b)\n",
    "print(f\"CUDA result: {result[0]}\")\n",
    "print(f\"NumPy result: {expected}\")\n",
    "print(f\"Match: {np.allclose(result[0], expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95acae5-ede3-4413-8ad6-3997374979dd",
   "metadata": {},
   "source": [
    "## Numba: Grid and Block Dimensions\n",
    "\n",
    "**Key concepts:**\n",
    "- `kernel[grid, block](args)` - Launch syntax\n",
    "- **Grid** = `(blocks_x, blocks_y, blocks_z)` - How many blocks\n",
    "- **Block** = `(threads_x, threads_y, threads_z)` - How many threads per block\n",
    "- Total threads = `grid_x √ó grid_y √ó grid_z √ó block_x √ó block_y √ó block_z`\n",
    "\n",
    "---\n",
    "\n",
    "## Triton: Program Grid\n",
    "\n",
    "In **Triton**, you specify a **program grid** and work with **program IDs**.\n",
    "\n",
    "**Key concepts:**\n",
    "- `kernel[grid](args, BLOCK_SIZE=...)` - Launch syntax\n",
    "- **Grid** = `(programs_x, programs_y, programs_z)` - Number of program instances\n",
    "- **No explicit block/thread dimensions** - Triton handles threading automatically\n",
    "- Work with **blocks of data** using `tl.arange()` and vectorized operations\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Aspect | Numba CUDA | Triton |\n",
    "|--------|------------|--------|\n",
    "| **Launch syntax** | `kernel[grid, block](args)` | `kernel[grid](args, BLOCK=...)` |\n",
    "| **Grid represents** | Number of **blocks** | Number of **programs** |\n",
    "| **Block/Thread control** | Explicit: `(tx, ty, tz)` per block | Abstracted: work on data blocks |\n",
    "| **Thread indexing** | Manual: `blockIdx`, `threadIdx` | Automatic: `tl.program_id()` + `tl.arange()` |\n",
    "| **Typical grid** | `(n_blocks_x, n_blocks_y, n_blocks_z)` | `(n_programs_x, n_programs_y, n_programs_z)` |\n",
    "| **Typical block** | `(threads_x, threads_y, threads_z)` | N/A (implicit in `BLOCK_SIZE`) |\n",
    "| **Memory access** | Per-thread indexing | Vectorized block operations |\n",
    "| **Abstraction level** | Low-level (like CUDA C) | High-level (compiler optimizes) |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "- **Numba CUDA**: You think in terms of **blocks of threads** (2-level hierarchy: grid ‚Üí blocks ‚Üí threads)\n",
    "- **Triton**: You think in terms of **programs operating on data blocks** (1-level: grid ‚Üí programs, with implicit vectorization)\n",
    "\n",
    "Triton is conceptually **one level of \"blocks\"** in CUDA terms - each Triton program is roughly equivalent to a CUDA block, but Triton automatically handles the thread-level parallelism within that program.\n",
    "\n",
    "## Practical Example: Vector Addition\n",
    "\n",
    "Below is the solution of a puzzle above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "342ddee6-0796-493d-99ea-93a44ed3bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# CUDA kernel with Guard\n",
    "@cuda.jit\n",
    "def zip_guard_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "SIZE = 1000\n",
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=out)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (SIZE + threads_per_block - 1) // threads_per_block\n",
    "zip_guard_kernel[blocks_per_grid, threads_per_block](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "830121a7-ecf5-4272-8737-3c9ac30b79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1173350e-99b7-4896-9106-d2dff7765435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def zip_guard_triton(a_ptr, b_ptr, out_ptr, n, BLOCK_SIZE: tl.constexpr):\n",
    "    # Triton uses program_id (block index) instead of explicit blockIdx/threadIdx\n",
    "    # Each \"program\" processes BLOCK_SIZE elements at once (vectorized)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Triton computes offsets for a *vector* of BLOCK_SIZE elements\n",
    "    # Unlike Numba where each thread processes 1 element,\n",
    "    # Triton processes multiple elements per program instance\n",
    "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Triton uses mask-based guards for vectorized operations\n",
    "    # instead of scalar if-statements (if i < size)\n",
    "    mask = offset < n\n",
    "    \n",
    "    # Vectorized load: loads BLOCK_SIZE elements at once with mask\n",
    "    # Numba loads scalar: a[i]\n",
    "    a = tl.load(a_ptr + offset, mask=mask)\n",
    "    b = tl.load(b_ptr + offset, mask=mask)\n",
    "    \n",
    "    # Vectorized computation (same as Numba but on vectors)\n",
    "    c = a + b\n",
    "    \n",
    "    # Vectorized store with mask (vs. scalar store in Numba)\n",
    "    tl.store(out_ptr + offset, c, mask=mask)\n",
    "\n",
    "\n",
    "# Triton works directly with PyTorch tensors (no manual copy_to_host)\n",
    "# Numba requires explicit device memory management (init_pb, copy_to_host)\n",
    "a = torch.randn(SIZE, device=get_device())\n",
    "b = torch.randn(SIZE, device=get_device())\n",
    "out = torch.empty_like(a)\n",
    "\n",
    "\n",
    "# Launch syntax differences:\n",
    "# - BLOCK_SIZE is a compile-time constant (tl.constexpr) for optimization\n",
    "# - Only need to specify grid dimensions (not threads_per_block)\n",
    "# - Triton auto-vectorizes within each program\n",
    "BLOCK_SIZE = 256\n",
    "grid = (triton.cdiv(SIZE, BLOCK_SIZE),)  # Only grid size, not block size\n",
    "zip_guard_triton[grid](a, b, out, SIZE, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Correct:  {np.allclose(out.cpu().numpy(), expected.cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b1fb5-ff7a-4730-8283-5c8beb9ef55b",
   "metadata": {},
   "source": [
    "Now you need to code the dot product with triton. You can use [`tl.sum` ](https://triton-lang.org/main/python-api/generated/triton.language.sum.html) and [`tl.atomic_add`](https://triton-lang.org/main/python-api/generated/triton.language.atomic_add.html#triton.language.atomic_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "86a72ef4-0b17-47e6-90e6-837363b56613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton result: 285.0\n",
      "PyTorch result: 285.0\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def dot_kernel(\n",
    "    a_ptr,      # Pointer to first input vector\n",
    "    b_ptr,      # Pointer to second input vector  \n",
    "    out_ptr,    # Pointer to output scalar\n",
    "    size,       # Size of vectors\n",
    "    BLOCK_SIZE: tl.constexpr,  # Elements per program\n",
    "):\n",
    "    # Program ID (analogous to blockIdx.x)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Compute offsets for this program's block\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Mask for boundary handling\n",
    "    mask = offsets < size\n",
    "    \n",
    "    # Load data \n",
    "    a = tl.load(a_ptr + offsets, mask=mask)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    products = a * b\n",
    "    \n",
    "    # Reduce sum within this block (automatic parallel reduction!)\n",
    "    block_sum = tl.sum(products)\n",
    "    \n",
    "    # Atomic add to output (single thread per block does this)\n",
    "    tl.atomic_add(out_ptr, block_sum)\n",
    "\n",
    "\n",
    "def dot_triton(a, b):\n",
    "    \"\"\"Wrapper function to launch the kernel\"\"\"\n",
    "    # Allocate output\n",
    "    out = torch.zeros(1, device=a.device, dtype=a.dtype)\n",
    "    \n",
    "    # Grid and block configuration\n",
    "    size = a.shape[0]\n",
    "    BLOCK_SIZE = 256  \n",
    "    grid = (triton.cdiv(size, BLOCK_SIZE),)\n",
    "    \n",
    "    # Launch kernel\n",
    "    dot_kernel[grid](a, b, out, size, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# Usage example\n",
    "SIZE = 10\n",
    "a = torch.arange(SIZE, dtype=torch.float32, device=get_device())\n",
    "b = torch.arange(SIZE, dtype=torch.float32, device=get_device())\n",
    "\n",
    "result_triton = dot_triton(a, b)\n",
    "result_torch = torch.dot(a, b)\n",
    "\n",
    "print(f\"Triton result: {result_triton.item()}\")\n",
    "print(f\"PyTorch result: {result_torch.item()}\")\n",
    "print(f\"Match: {torch.allclose(result_triton, result_torch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff070e-1cd8-47f7-9748-afb130d1681a",
   "metadata": {},
   "source": [
    "Now, you need to code the softmax function for a batch of vectors z of size `bs x d` we want to compute `torch.softmax(z, axis=1)`.\n",
    "**Visual:**\n",
    "```\n",
    "Memory: [a, b, c, d, e, f, g, h, i, j, k, l]\n",
    "Shape (3, 4), stride (4, 1):\n",
    "  [[a, b, c, d],    ‚Üê skip 1 for next col, skip 4 for next row\n",
    "   [e, f, g, h],\n",
    "   [i, j, k, l]]\n",
    "```\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bd3b901c-56ee-4236-b561-93a2e81d80e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contiguous 2D tensor (3√ó4)\n",
    "x = torch.randn(3, 4)\n",
    "x.stride()  # (4, 1)\n",
    "# - Move to next row: skip 4 elements\n",
    "# - Move to next column: skip 1 element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aa9c0f44-1ac9-43a3-b9ef-9d21e6b895a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposed (now 4√ó3)\n",
    "y = x.t()\n",
    "y.stride()  # (1, 4)\n",
    "# - Move to next row: skip 1 element (was column)\n",
    "# - Move to next column: skip 4 elements (was row)\n",
    "# Note: y shares memory with x, just different access pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e4efb-5ddc-4b65-87a4-b647a6cba214",
   "metadata": {},
   "source": [
    "**Why it matters:**\n",
    "For our softmax function, we assume that each program can process one full row independently. The first task will be to locate the start of the corresponding row in memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "380315a5-3677-4a59-be40-45f942bc6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def triton_softmax_kernel(x_ptr, y_ptr, x_row_stride, y_row_stride, num_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    assert num_cols <= BLOCK_SIZE\n",
    "    # Process each row independently\n",
    "    row_idx = tl.program_id(0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    # Read from global memory\n",
    "    x_start_ptr = x_ptr + row_idx * x_row_stride\n",
    "    x_ptrs = x_start_ptr + col_offsets\n",
    "    x_row = tl.load(x_ptrs, mask=col_offsets < num_cols, other=float(\"-inf\"))\n",
    "    # Compute\n",
    "    x_row = x_row - tl.max(x_row, axis=0)\n",
    "    numerator = tl.exp(x_row)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    y_row = numerator / denominator\n",
    "    # Write back to global memory\n",
    "    y_start_ptr = y_ptr + row_idx * y_row_stride\n",
    "    y_ptrs = y_start_ptr + col_offsets\n",
    "    tl.store(y_ptrs, y_row, mask=col_offsets < num_cols)\n",
    "\n",
    "def triton_softmax(x: torch.Tensor):\n",
    "    x = x.contiguous()\n",
    "    # Allocate output tensor\n",
    "    y = torch.empty_like(x)\n",
    "    # Determine grid\n",
    "    M, N = x.shape                          # Number of rows x number of columns\n",
    "    block_size = triton.next_power_of_2(N)  # Each block contains all the columns\n",
    "    num_blocks = M                          # Each block is a row\n",
    "    # Launch kernel\n",
    "    triton_softmax_kernel[(M,)](\n",
    "        x_ptr=x, y_ptr=y,\n",
    "        x_row_stride=x.stride(0), y_row_stride=y.stride(0),\n",
    "        num_cols=N, BLOCK_SIZE=block_size\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f1735d03-3bfa-41b5-8b4c-67235974d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device=get_device())\n",
    "y_triton = triton_softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "89635810-32e9-4198-8a3e-fb168eed7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_triton = triton_softmax(x.t())\n",
    "y_torch = torch.softmax(x.t(), axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ddc34a21-9928-44d1-820b-4fdb1518a250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = get_device()\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch\"],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    stream = getattr(torch, DEVICE.type).Stream()\n",
    "    getattr(torch, DEVICE.type).set_stream(stream)\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: triton_softmax(x))\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3c03191b-645c-4c91-9423-f5ea74b3137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,  # Input pointers\n",
    "    output_ptr,  # Output pointer\n",
    "    x_stride_row, x_stride_dim,  # Strides tell us how to move one element in each axis of a tensor\n",
    "    weight_stride_dim,  # Likely 1\n",
    "    output_stride_row,  # Likely 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # Tile shapes must be known at compile time\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a tile of rows of x.\n",
    "    # `tl.program_id` gives us a way to check which thread block we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    \n",
    "    # Block pointers give us a way to select from an ND region of memory\n",
    "    # and move our selection around.\n",
    "    # The block pointer must know:\n",
    "    # - The pointer to the first element of the tensor\n",
    "    # - The overall shape of the tensor to handle out-of-bounds access\n",
    "    # - The strides of each dimension to use the memory layout properly\n",
    "    # - The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "    # - The block shape to use load/store at a time\n",
    "    # - The order of the dimensions in memory from major to minor\n",
    "    # axes (= np.argsort(strides)) for optimizations, especially useful on H100\n",
    "    \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    # Initialize a buffer to write to\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "    \n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        # Load the current block pointer\n",
    "        # Since ROWS_TILE_SIZE might not divide ROWS, and D_TILE_SIZE might not divide D,\n",
    "        # we need boundary checks for both dimensions\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "        \n",
    "        # Compute the weighted sum of the row.\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "        \n",
    "        # Move the pointers to the next tile.\n",
    "        # These are (rows, columns) coordinate deltas\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # Move by D_TILE_SIZE in the last dimension\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # Move by D_TILE_SIZE\n",
    "    \n",
    "    # Write output to the output block pointer (a single scalar per row).\n",
    "    # Since ROWS_TILE_SIZE might not divide ROWS, we need boundary checks\n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1707e502-0d50-4ecb-94fb-bd5850e078e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_triton(x: torch.Tensor, weight: torch.Tensor):\n",
    "    D = x.shape[-1]\n",
    "    output_dims = x.shape[:-1]   \n",
    "    # Reshape input tensor to 2D\n",
    "    \n",
    "    x = rearrange(x, \"... d -> (...) d\")\n",
    "    # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "    y = torch.empty(x.shape[0], device=x.device)\n",
    "\n",
    "    D_TILE_SIZE = triton.next_power_of_2(D) // 16  # Roughly 16 loops through the embedding dimension\n",
    "    ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "        \n",
    "    # Launch our kernel with n instances in our 1D grid.\n",
    "    n_rows = y.numel()\n",
    "    weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "        \n",
    "    return y.view(output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3bd16f2a-719a-4bfe-a254-5c40ee7bd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal3(f1, f2):\n",
    "    x = torch.randn(64, 64, 2048, device=get_device())\n",
    "    w = torch.randn(2048, device=get_device())\n",
    "    y1 = f1(x,w)\n",
    "    y2 = f2(x,w)\n",
    "    assert torch.allclose(y1, y2, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8c361f23-92f7-4543-90f7-07be6e6bdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_equal3(weighted_sum,weighted_sum_triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d3769993-6ad7-45fe-8e4b-df074b8708a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # Cache x and weight to be used in the backward pass, when we\n",
    "        # only receive the gradient wrt. the output tensor, and\n",
    "        # need to compute the gradients wrt. x and weight.\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "\n",
    "        # Reshape input tensor to 2D\n",
    "        x_reshaped = rearrange(x, \"... d -> (...) d\")\n",
    "\n",
    "        #ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
    "        assert (\n",
    "            x_reshaped.is_contiguous()\n",
    "        ), \"Our pointer arithmetic will assume contiguous x\"\n",
    "\n",
    "        D_TILE_SIZE = (\n",
    "            triton.next_power_of_2(D) // 16\n",
    "        )  # Roughly 16 loops through the embedding dimension\n",
    "        ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "\n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "        y = torch.empty(x_reshaped.shape[0], device=x.device)\n",
    "\n",
    "        # Launch our kernel with n instances in our 1D grid.\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x_reshaped,\n",
    "            weight,\n",
    "            y,\n",
    "            x_reshaped.stride(0),\n",
    "            x_reshaped.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows,\n",
    "            D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE,\n",
    "            D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        return y.view(output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8a0d36e3-3c59-42ab-87a0-de51df85bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weightedsum = torch.compile(WeightedSumFunc.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ab2ecb29-b3f1-4ba9-84da-eac6eb445ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.4106], device='cuda:0', grad_fn=<WeightedSumFuncBackward>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 16, device=get_device())\n",
    "w = torch.randn(16, device=get_device()).requires_grad_(True)\n",
    "f_weightedsum(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ea0b14bf-2733-4732-9d7a-2c1795b20891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionTriton(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear regression using the custom Triton weighted sum kernel.\n",
    "\n",
    "    Model: y = w^T x + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(input_dim, device=\"cuda\") * 0.01)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1, device=\"cuda\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        # Use our custom weighted sum function\n",
    "        return f_weightedsum(x, self.weight) + self.bias\n",
    "\n",
    "\n",
    "def generate_regression_data(n_samples=1000, input_dim=128, noise_std=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic linear regression data.\n",
    "\n",
    "    Returns:\n",
    "        X: (n_samples, input_dim) feature matrix\n",
    "        y: (n_samples,) continuous target values\n",
    "        true_weight: (input_dim,) true weight vector used for generation\n",
    "        true_bias: (1,) true bias value used for generation\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate random features\n",
    "    X = torch.randn(n_samples, input_dim, device=\"cuda\")\n",
    "\n",
    "    # Create true weights for data generation\n",
    "    true_weight = torch.randn(input_dim, device=\"cuda\")\n",
    "    true_bias = torch.randn(1, device=\"cuda\")\n",
    "\n",
    "    # Generate target values: y = w^T x + b + noise\n",
    "    y = X @ true_weight + true_bias\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    noise = torch.randn(n_samples, device=\"cuda\") * noise_std\n",
    "    y = y + noise\n",
    "\n",
    "    return X, y, true_weight, true_bias\n",
    "\n",
    "\n",
    "def train_linear_regression(\n",
    "    model, X_train, y_train, X_val, y_val, epochs=100, lr=0.01, batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the linear regression model.\n",
    "\n",
    "    Args:\n",
    "        model: LinearRegressionTriton model\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        batch_size: Batch size for training\n",
    "\n",
    "    Returns:\n",
    "        train_losses: List of training losses (MSE) per epoch\n",
    "        val_losses: List of validation losses (MSE) per epoch\n",
    "        val_r2_scores: List of validation R¬≤ scores per epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_r2_scores = []\n",
    "\n",
    "    n_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(len(X_train), device=\"cuda\")\n",
    "        X_train_shuffled = X_train[perm]\n",
    "        y_train_shuffled = y_train[perm]\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(X_train))\n",
    "\n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Calculate R¬≤ score\n",
    "            ss_res = ((y_val - y_val_pred) ** 2).sum()\n",
    "            ss_tot = ((y_val - y_val.mean()) ** 2).sum()\n",
    "            r2_score = 1 - (ss_res / ss_tot)\n",
    "            val_r2_scores.append(r2_score.item())\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                f\"Train Loss = {avg_train_loss:.4f}, \"\n",
    "                f\"Val Loss = {val_loss:.4f}, \"\n",
    "                f\"Val R¬≤ = {r2_score.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    return train_losses, val_losses, val_r2_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cce9df3c-9b9d-48d2-a6b7-37ddd2d142ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate linear regression with custom Triton kernel.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Linear Regression with Custom Triton Weighted Sum Kernel\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Check CUDA availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ERROR: CUDA is not available. This example requires a GPU.\")\n",
    "        return\n",
    "\n",
    "    # Hyperparameters\n",
    "    n_train = 800\n",
    "    n_val = 200\n",
    "    input_dim = 128\n",
    "    epochs = 100\n",
    "    lr = 0.01\n",
    "    batch_size = 64\n",
    "    noise_std = 0.1\n",
    "\n",
    "    print(f\"\\nDataset configuration:\")\n",
    "    print(f\"  Training samples: {n_train}\")\n",
    "    print(f\"  Validation samples: {n_val}\")\n",
    "    print(f\"  Input dimension: {input_dim}\")\n",
    "    print(f\"  Noise std: {noise_std}\")\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Learning rate: {lr}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print()\n",
    "\n",
    "    # Generate data\n",
    "    print(\"Generating synthetic linear regression data...\")\n",
    "    X, y, true_weight, true_bias = generate_regression_data(\n",
    "        n_samples=n_train + n_val, input_dim=input_dim, noise_std=noise_std\n",
    "    )\n",
    "\n",
    "    # Split into train and validation\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:], y[n_train:]\n",
    "\n",
    "    print(f\"Training set: X shape = {X_train.shape}, y shape = {y_train.shape}\")\n",
    "    print(f\"Validation set: X shape = {X_val.shape}, y shape = {y_val.shape}\")\n",
    "    print(f\"\\nTrue parameters:\")\n",
    "    print(f\"  True weight norm: {true_weight.norm().item():.6f}\")\n",
    "    print(f\"  True bias: {true_bias.item():.6f}\")\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing linear regression model with Triton kernel...\")\n",
    "    model = LinearRegressionTriton(input_dim=input_dim)\n",
    "    print(\n",
    "        f\"Model parameters: weight shape = {model.weight.shape}, bias shape = {model.bias.shape}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\\n\")\n",
    "    train_losses, val_losses, val_r2_scores = train_linear_regression(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Final Training Loss (MSE): {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss (MSE): {val_losses[-1]:.4f}\")\n",
    "    print(f\"Final Validation R¬≤: {val_r2_scores[-1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Compare learned parameters with true parameters\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Parameter Comparison: Learned vs True\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    learned_weight = model.weight.data\n",
    "    learned_bias = model.bias.data\n",
    "\n",
    "    # Compute various comparison metrics\n",
    "    weight_diff = learned_weight - true_weight\n",
    "    weight_mse = (weight_diff**2).mean().item()\n",
    "    weight_mae = weight_diff.abs().mean().item()\n",
    "    weight_cos_sim = torch.nn.functional.cosine_similarity(\n",
    "        learned_weight.unsqueeze(0), true_weight.unsqueeze(0)\n",
    "    ).item()\n",
    "\n",
    "    bias_diff = (learned_bias - true_bias).abs().item()\n",
    "\n",
    "    # Compute correlation\n",
    "    weight_corr = torch.corrcoef(torch.stack([learned_weight, true_weight]))[\n",
    "        0, 1\n",
    "    ].item()\n",
    "\n",
    "    print(f\"\\nWeight Statistics:\")\n",
    "    print(f\"  True weight norm:        {true_weight.norm().item():.6f}\")\n",
    "    print(f\"  Learned weight norm:     {learned_weight.norm().item():.6f}\")\n",
    "    print(f\"  Weight MSE:              {weight_mse:.6f}\")\n",
    "    print(f\"  Weight MAE:              {weight_mae:.6f}\")\n",
    "    print(f\"  Weight cosine similarity: {weight_cos_sim:.6f}\")\n",
    "    print(f\"  Weight correlation:      {weight_corr:.6f}\")\n",
    "\n",
    "    print(f\"\\nBias Statistics:\")\n",
    "    print(f\"  True bias:               {true_bias.item():.6f}\")\n",
    "    print(f\"  Learned bias:            {learned_bias.item():.6f}\")\n",
    "    print(f\"  Bias absolute difference: {bias_diff:.6f}\")\n",
    "\n",
    "    # Show top 5 weight components comparison\n",
    "    print(f\"\\nTop 5 Weight Components (by absolute value of true weight):\")\n",
    "    top_indices = true_weight.abs().argsort(descending=True)[:5]\n",
    "    print(f\"  {'Index':<8} {'True':<12} {'Learned':<12} {'Difference':<12}\")\n",
    "    print(f\"  {'-'*8} {'-'*12} {'-'*12} {'-'*12}\")\n",
    "    for idx in top_indices:\n",
    "        idx_val = idx.item()\n",
    "        true_val = true_weight[idx].item()\n",
    "        learned_val = learned_weight[idx].item()\n",
    "        diff = learned_val - true_val\n",
    "        print(f\"  {idx_val:<8} {true_val:>12.6f} {learned_val:>12.6f} {diff:>12.6f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Test gradient computation\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Gradient Computation Test\")\n",
    "    print(\"=\" * 80)\n",
    "    model.eval()\n",
    "\n",
    "    # Clear any existing gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    X_test = torch.randn(10, input_dim, device=\"cuda\", requires_grad=True)\n",
    "    y_test_target = torch.randn(10, device=\"cuda\")\n",
    "    y_pred = model(X_test)\n",
    "    loss = torch.nn.MSELoss()(y_pred, y_test_target)\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"‚úì Gradients computed successfully!\")\n",
    "    print(f\"  Test loss: {loss.item():.6f}\")\n",
    "    print(f\"  Weight gradient norm: {model.weight.grad.norm().item():.6f}\")\n",
    "    print(f\"  Bias gradient norm: {model.bias.grad.norm().item():.6f}\")\n",
    "    if X_test.grad is not None:\n",
    "        print(f\"  Input gradient norm: {X_test.grad.norm().item():.6f}\")\n",
    "    print()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Example completed successfully!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7e789074-7bd9-4ec6-8c31-eacda7522441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Linear Regression with Custom Triton Weighted Sum Kernel\n",
      "================================================================================\n",
      "\n",
      "Dataset configuration:\n",
      "  Training samples: 800\n",
      "  Validation samples: 200\n",
      "  Input dimension: 128\n",
      "  Noise std: 0.1\n",
      "\n",
      "Training configuration:\n",
      "  Epochs: 100\n",
      "  Learning rate: 0.01\n",
      "  Batch size: 64\n",
      "\n",
      "Generating synthetic linear regression data...\n",
      "Training set: X shape = torch.Size([800, 128]), y shape = torch.Size([800])\n",
      "Validation set: X shape = torch.Size([200, 128]), y shape = torch.Size([200])\n",
      "\n",
      "True parameters:\n",
      "  True weight norm: 10.257113\n",
      "  True bias: -0.518749\n",
      "\n",
      "Initializing linear regression model with Triton kernel...\n",
      "Model parameters: weight shape = torch.Size([128]), bias shape = torch.Size([1])\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "You must implement either the backward or vjp method for your custom autograd.Function to use it with backward mode AD.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m train_losses, val_losses, val_r2_scores = \u001b[43mtrain_linear_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[143]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mtrain_linear_regression\u001b[39m\u001b[34m(model, X_train, y_train, X_val, y_val, epochs, lr, batch_size)\u001b[39m\n\u001b[32m     96\u001b[39m loss = criterion(y_pred, y_batch)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m optimizer.step()\n\u001b[32m    102\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs336-syst/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs336-syst/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs336-syst/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs336-syst/lib/python3.11/site-packages/torch/autograd/function.py:307\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m     )\n\u001b[32m    306\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs336-syst/lib/python3.11/site-packages/torch/autograd/function.py:431\u001b[39m, in \u001b[36m_SingleLevelFunction.backward\u001b[39m\u001b[34m(ctx, *grad_outputs)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(ctx: Any, *grad_outputs: Any) -> Any:\n\u001b[32m    410\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define a formula for differentiating the operation with backward mode automatic differentiation.\u001b[39;00m\n\u001b[32m    411\u001b[39m \n\u001b[32m    412\u001b[39m \u001b[33;03m    This function is to be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m \u001b[33;03m    output.\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    432\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou must implement either the backward or vjp method for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    433\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myour custom autograd.Function to use it with backward \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    434\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmode AD.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    435\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: You must implement either the backward or vjp method for your custom autograd.Function to use it with backward mode AD."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e7afc-faa3-4cd9-a51b-4603c6598e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-syst",
   "language": "python",
   "name": "cs336-syst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
