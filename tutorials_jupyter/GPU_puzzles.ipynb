{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e256bcb-6387-4811-9959-f448426b42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cd6e1d-a546-4cef-b9a4-d0f42e24ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", category=numba.NumbaPerformanceWarning, module=\"numba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2911024-cc0b-46e9-9685-e1e6710f5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_spec(a):\n",
    "    return a + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a463ab-87e8-4a84-9c59-793297551b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    [0. 1. 2. 3.]\n",
      "Output:   [10. 11. 12. 13.]\n",
      "Expected: [10. 11. 12. 13.]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def map_kernel(out, a):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # Each thread adds 10 to one element\n",
    "    out[i] = a[i] + 10\n",
    "\n",
    "# Size of our array\n",
    "SIZE = 4\n",
    "\n",
    "# Create input and output arrays\n",
    "a = np.arange(SIZE, dtype=np.float32)  # [0, 1, 2, 3]\n",
    "out = np.zeros(SIZE, dtype=np.float32)\n",
    "\n",
    "# Copy arrays to GPU\n",
    "a_device = cuda.to_device(a)\n",
    "out_device = cuda.to_device(out)\n",
    "\n",
    "# kernel[grid, block](args)\n",
    "# Launch kernel: grid = 1 block, block = SIZE threads\n",
    "map_kernel[1, SIZE](out_device, a_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Input:    {a}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b8fcdf-cd27-4bb3-becd-60661a021407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_spec(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72eb9285-a415-476e-9201-5a368ba6f514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "zip_spec(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90295c2d-1514-46f5-a755-10cd76e7e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a:  [0 1 2 3]\n",
      "Input b:  [0 1 2 3]\n",
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def zip_kernel(out, a, b):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    \n",
    "    out[i] = a[i] + b[i]\n",
    "\n",
    "def init_pb(a=a, b=b, out=out):\n",
    "    a_device = cuda.to_device(a)\n",
    "    b_device = cuda.to_device(b)\n",
    "    out_device = cuda.to_device(out)\n",
    "    return a_device, b_device, out_device\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "# Launch kernel: 1 block, SIZE threads\n",
    "zip_kernel[1, SIZE](out_device, a_device, b_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Input a:  {a}\")\n",
    "print(f\"Input b:  {b}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47d9b2f-d542-4286-aa78-19152ce85474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a:  [0 1 2 3]\n",
      "Input b:  [0 1 2 3]\n",
      "Output:   [0. 2. 4. 6.]\n",
      "Expected: [0 2 4 6]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "# CUDA kernel with Guard\n",
    "@cuda.jit\n",
    "def zip_guard_kernel(out, a, b, size):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "NUM_TRHEADS = 2*SIZE\n",
    "zip_guard_kernel[1, NUM_TRHEADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Input a:  {a}\")\n",
    "print(f\"Input b:  {b}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35e349e-0d40-43a0-b409-79ad7836d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 11, 12, 13],\n",
       "       [14, 15, 16, 17],\n",
       "       [18, 19, 20, 21],\n",
       "       [22, 23, 24, 25]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))\n",
    "out = map_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948815aa-6a9e-428d-b472-f9858ef1dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[10 11 12 13]\n",
      " [14 15 16 17]\n",
      " [18 19 20 21]\n",
      " [22 23 24 25]]\n",
      "Expected: [[10 11 12 13]\n",
      " [14 15 16 17]\n",
      " [18 19 20 21]\n",
      " [22 23 24 25]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def map_2d_kernel(out, a, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,j] + 10\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "NUM_TRHEADS = (SIZE, SIZE)\n",
    "map_2d_kernel[1, NUM_TRHEADS](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8bc22dc-5282-4687-a058-c14441863971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [1, 2, 3, 4],\n",
       "       [2, 3, 4, 5],\n",
       "       [3, 4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(SIZE).reshape(SIZE, 1)\n",
    "b = np.arange(SIZE).reshape(1, SIZE)\n",
    "out = a + b\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858dcc2a-20ed-472c-8125-7621ea5c8c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_kernel(out, a, b, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = (2*SIZE, SIZE)\n",
    "broadcast_kernel[1, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed23d449-fcd6-44d1-a150-ea58779f1798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x \n",
    "    j = cuda.blockIdx.y \n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "# 1D threads, 2D grid\n",
    "THREADS = 1\n",
    "GRID = (SIZE, SIZE)\n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2281d6cd-4cc4-45f9-859c-e3dda4586a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE\n",
    "GRID = (1, SIZE)  \n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "299c4531-3b33-4bab-a76e-6d8592db7f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIZE//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea2afc6-c4c4-4276-a79e-190ade51d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Expected: [[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n",
    "    if i < size and j < size:\n",
    "        out[i,j] = a[i,0] + b[0,j]\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = (SIZE//2 , SIZE // 2)\n",
    "GRID = (SIZE//2, SIZE//2)  \n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e0e2a-602b-4d0b-b4b0-abef682330dc",
   "metadata": {},
   "source": [
    "`blockDim.x = blockDim.y =2 `\n",
    "\n",
    "| blockIdx.x | blockIdx.y  | threadIdx.x | threadIdx.y | **i** | **j** | Computes |\n",
    "|------------|---------------|-------------|-------------|-------|-------|----------|\n",
    "| 0 | 0 | 0 | 0 | **0** | **0** | out[0,0] |\n",
    "| 0 | 0 | 1 | 0 | **1** | **0** | out[1,0] |\n",
    "| 0 | 0 | 0 | 1 | **0** | **1** | out[0,1] |\n",
    "| 0 | 0 | 1 | 1 | **1** | **1** | out[1,1] |\n",
    "| 1 | 0 | 0 | 0 | **2** | **0** | out[2,0] |\n",
    "| 1 | 0 | 1 | 0 | **3** | **0** | out[3,0] |\n",
    "| 0 | 1 | 0 | 0 | **0** | **2** | out[0,2] |\n",
    "| 0 | 1 | 1 | 1 | **1** | **3** | out[1,3] |\n",
    "| 1 | 1 | 0 | 0 | **2** | **2** | out[2,2] |\n",
    "| 1 | 1 | 1 | 1 | **3** | **3** | out[3,3] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7633ae72-85e7-4196-bde1-fd187b750b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  3.,  6.,  9., 12., 15., 18.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool_spec(a):\n",
    "    out = np.zeros(a.shape)\n",
    "    for i in range(a.shape[0]):\n",
    "        out[i] = a[max(i - 2, 0) : i + 1].sum()\n",
    "    return out\n",
    "\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE)\n",
    "out = pool_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9017cc-7569-48d5-89e0-fc931f89126a",
   "metadata": {},
   "source": [
    "```\n",
    "Memory hierarchy:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Global Memory (a, out)             â”‚  â† ALL threads can access\n",
    "â”‚  - Slow                             â”‚\n",
    "â”‚  - Accessible across all blocks     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“                    â†“\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Block 0 â”‚          â”‚ Block 1 â”‚\n",
    "   â”‚ Shared  â”‚          â”‚ Shared  â”‚      â† Only threads in THIS block\n",
    "   â”‚ Memory  â”‚          â”‚ Memory  â”‚         can access\n",
    "   â”‚ (fast)  â”‚          â”‚ (fast)  â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ac15df9-9dcd-4493-bf31-571f774801a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Expected: [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Correct:  True\n",
      "number of access to global memory: 1 + 2 + 2 threads x 3 reads = 9 global reads per block -> 18 global reads in total\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def pool_kernel(out, a, size):\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        # Manually compute sum - can't use slicing in CUDA!\n",
    "        temp_sum = 0.0\n",
    "        for k in range(max(i - 2, 0), i + 1):\n",
    "            temp_sum += a[k]  # Use global memory to handle cross-block boundaries\n",
    "        out[i] = temp_sum\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE//2\n",
    "GRID = (2,1)  \n",
    "pool_kernel[GRID, THREADS](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")\n",
    "r = 3\n",
    "print(f\"number of access to global memory: 1 + 2 + {THREADS-2} threads x {r} reads = {1+2+(THREADS-2)*r} global reads per block -> {2*(1+2+(THREADS-2)*r)} global reads in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c12f84-4fac-49c3-b2bd-e1f5322e18ba",
   "metadata": {},
   "source": [
    "The method below allows for 8+4=12 global reads in total.\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Global:      [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "Block 0 loads:              Block 1 loads:\n",
    "        â†“                          â†“\n",
    "Shared: [0, 0, 0, 1, 2, 3] Shared: [2, 3, 4, 5, 6, 7]\n",
    "         â””â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        halo    main data          halo    main data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfcc95e-f679-445f-babe-c1acca3a86ee",
   "metadata": {},
   "source": [
    "Each block can only have a constant amount of shared memory that threads in that block can read and write to. This needs to be a literal python constant not a variable. After writing to shared memory you need to call `cuda.syncthreads` to ensure that threads do not cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92a76dde-423b-459d-8027-4eb99c3a0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Expected: [ 0.  1.  3.  6.  9. 12. 15. 18.]\n",
      "Correct:  True\n"
     ]
    }
   ],
   "source": [
    "TPB = 4  # Threads per block\n",
    "SharedMem = TPB + 2 # cannot computed at runtime\n",
    "@cuda.jit\n",
    "def pool_kernel_shared(out, a, size):\n",
    "    # Allocate shared memory with HALO (extra elements for boundary)\n",
    "    # Need TPB + 2 extra elements (for the 2-element lookback)\n",
    "    shared = cuda.shared.array(SharedMem, numba.float32)\n",
    "    \n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    local_i = cuda.threadIdx.x\n",
    "    \n",
    "    # Each thread loads its own element into shared memory (offset by 2 for halo)\n",
    "    if i < size:\n",
    "        shared[local_i + 2] = a[i]\n",
    "    \n",
    "    # First 2 threads also load the HALO (left boundary elements)\n",
    "    if local_i == 0:\n",
    "        # Load 2 elements before the block starts\n",
    "        start_idx = cuda.blockIdx.x * cuda.blockDim.x\n",
    "        if start_idx >= 2:\n",
    "            shared[1] = a[start_idx - 1]\n",
    "            shared[0] = a[start_idx - 2]\n",
    "        else:\n",
    "            shared[1] = 0.0\n",
    "            shared[0] = 0.0 # Padding for out-of-bounds\n",
    "            \n",
    "    # Wait for all threads to finish loading\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Now compute using shared memory\n",
    "    if i < size:\n",
    "        temp_sum = 0.0\n",
    "        # Look back up to 2 elements in shared memory\n",
    "        for k in range(max(0, 3 - (i + 1)), 3):  # At most 3 elements\n",
    "            temp_sum += shared[local_i + 2 - (2 - k)]\n",
    "        out[i] = temp_sum\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "\n",
    "THREADS = TPB\n",
    "GRID = (SIZE // TPB, 1)  # (2, 1) for SIZE=8, TPB=4\n",
    "pool_kernel_shared[GRID, THREADS](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6243367-7197-49b6-a6d9-668a920fe1a9",
   "metadata": {},
   "source": [
    "![](https://www.cs.uaf.edu/2012/fall/cs441/lecture/tree_sum_16td.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a6919a7-95f1-4887-b86d-667784b92405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA result: 140.0\n",
      "NumPy result: 140.0\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def dot_kernel_numba(a, b, out, size):\n",
    "    # â† At this point, a, b, out are ALREADY on the device (GPU)\n",
    "    # This kernel executes on the GPU\n",
    "    shared = cuda.shared.array(256, numba.float32)\n",
    "    \n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    local_i = cuda.threadIdx.x\n",
    "    \n",
    "    if i < size:\n",
    "        shared[local_i] = a[i] * b[i]\n",
    "    else:\n",
    "        shared[local_i] = 0.0\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if local_i < stride:\n",
    "            shared[local_i] += shared[local_i + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "    \n",
    "    if local_i == 0:\n",
    "        cuda.atomic.add(out, 0, shared[0]) # To avoid RACE CONDITION! Multiple blocks write to out[0] see below\n",
    "\n",
    "# Test\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "\n",
    "expected = np.dot(a, b)\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like([expected]))\n",
    "\n",
    "\n",
    "size = a_device.shape[0]\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "# Launch kernel - a_device, b_device, out_device are all on GPU\n",
    "dot_kernel_numba[blocks_per_grid, threads_per_block](a_device, b_device, out_device, size)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "    \n",
    "print(f\"CUDA result: {result[0]}\")\n",
    "print(f\"NumPy result: {expected}\")\n",
    "print(f\"Match: {np.allclose(result[0], expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaffa23-43cd-4bcf-aa9d-d291dc1f0df5",
   "metadata": {},
   "source": [
    "## The Problem Without Atomics\n",
    "When you write out[0] = out[0] + value, it's actually 3 separate steps:\n",
    "```\n",
    "# out[0] = out[0] + value breaks down to:\n",
    "1. READ:   temp = out[0]      # Read current value\n",
    "2. MODIFY: temp = temp + value # Add to it\n",
    "3. WRITE:  out[0] = temp       # Write back\n",
    "```\n",
    "\n",
    "**With multiple threads, these steps can interleave and lose updates:**\n",
    "```\n",
    "Initial: out[0] = 0\n",
    "\n",
    "Thread A (Block 0):              Thread B (Block 1):\n",
    "1. READ: temp_A = 0             \n",
    "2. MODIFY: temp_A = 0 + 5       \n",
    "                                 1. READ: temp_B = 0      â† Still sees 0!\n",
    "3. WRITE: out[0] = 5            \n",
    "                                 2. MODIFY: temp_B = 0 + 3 â† Uses old value!\n",
    "                                 3. WRITE: out[0] = 3      â† Overwrites 5!\n",
    "\n",
    "Final: out[0] = 3  âŒ Should be 8!\n",
    "```\n",
    "\n",
    "## What Atomics Do\n",
    "\n",
    "`cuda.atomic.add(out, 0, value)` **locks the memory location** so the entire operation completes before another thread can access it:\n",
    "```\n",
    "Initial: out[0] = 0\n",
    "\n",
    "Thread A (Block 0):              Thread B (Block 1):\n",
    "ðŸ”’ LOCK out[0]\n",
    "1. READ: temp_A = 0             \n",
    "2. MODIFY: temp_A = 0 + 5       \n",
    "3. WRITE: out[0] = 5            \n",
    "ðŸ”“ UNLOCK out[0]\n",
    "                                 ðŸ”’ LOCK out[0]  â† Must wait for unlock\n",
    "                                 1. READ: temp_B = 5      â† Sees updated value!\n",
    "                                 2. MODIFY: temp_B = 5 + 3\n",
    "                                 3. WRITE: out[0] = 8\n",
    "                                 ðŸ”“ UNLOCK out[0]\n",
    "\n",
    "Final: out[0] = 8  âœ… Correct!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95acae5-ede3-4413-8ad6-3997374979dd",
   "metadata": {},
   "source": [
    "## Numba CUDA: Grid and Block Dimensions\n",
    "\n",
    "In **Numba CUDA**, you launch kernels with explicit grid and block dimensions:\n",
    "```python\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def kernel(output):\n",
    "    # Thread indices within the block\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    tz = cuda.threadIdx.z\n",
    "    \n",
    "    # Block indices within the grid\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    bz = cuda.blockIdx.z\n",
    "    \n",
    "    # Block dimensions\n",
    "    block_dim_x = cuda.blockDim.x\n",
    "    block_dim_y = cuda.blockDim.y\n",
    "    \n",
    "    # Global thread index\n",
    "    i = bx * block_dim_x + tx\n",
    "    j = by * block_dim_y + ty\n",
    "    \n",
    "    output[i, j] = i * 1000 + j\n",
    "\n",
    "# Launch configuration\n",
    "threads_per_block = (16, 16)  # Block dimensions: 16x16 = 256 threads per block\n",
    "blocks_per_grid = (4, 8)       # Grid dimensions: 4x8 = 32 blocks total\n",
    "\n",
    "output = np.zeros((64, 128), dtype=np.int32)\n",
    "kernel[blocks_per_grid, threads_per_block](output)\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "- `kernel[grid, block](args)` - Launch syntax\n",
    "- **Grid** = `(blocks_x, blocks_y, blocks_z)` - How many blocks\n",
    "- **Block** = `(threads_x, threads_y, threads_z)` - How many threads per block\n",
    "- Total threads = `grid_x Ã— grid_y Ã— grid_z Ã— block_x Ã— block_y Ã— block_z`\n",
    "\n",
    "---\n",
    "\n",
    "## Triton: Program Grid\n",
    "\n",
    "In **Triton**, you specify a **program grid** and work with **program IDs**:\n",
    "```python\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def kernel(output_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n",
    "    # Program ID (like block index in CUDA)\n",
    "    pid_x = tl.program_id(0)\n",
    "    pid_y = tl.program_id(1)\n",
    "    \n",
    "    # Compute offsets for this program\n",
    "    row_start = pid_x * BLOCK_SIZE\n",
    "    col_start = pid_y * BLOCK_SIZE\n",
    "    \n",
    "    # Create a block of indices\n",
    "    rows = row_start + tl.arange(0, BLOCK_SIZE)\n",
    "    cols = col_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Triton handles the actual thread mapping automatically\n",
    "    output = rows[:, None] * 1000 + cols[None, :]\n",
    "    \n",
    "    # Store results\n",
    "    mask = (rows[:, None] < M) & (cols[None, :] < N)\n",
    "    tl.store(output_ptr + rows[:, None] * N + cols[None, :], output, mask=mask)\n",
    "\n",
    "# Launch configuration\n",
    "M, N = 64, 128\n",
    "BLOCK_SIZE = 16\n",
    "\n",
    "output = torch.zeros((M, N), dtype=torch.int32, device='cuda')\n",
    "\n",
    "# Grid: number of programs in each dimension\n",
    "grid = (triton.cdiv(M, BLOCK_SIZE), triton.cdiv(N, BLOCK_SIZE))\n",
    "kernel[grid](output, M, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "- `kernel[grid](args, BLOCK_SIZE=...)` - Launch syntax\n",
    "- **Grid** = `(programs_x, programs_y, programs_z)` - Number of program instances\n",
    "- **No explicit block/thread dimensions** - Triton handles threading automatically\n",
    "- Work with **blocks of data** using `tl.arange()` and vectorized operations\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Aspect | Numba CUDA | Triton |\n",
    "|--------|------------|--------|\n",
    "| **Launch syntax** | `kernel[grid, block](args)` | `kernel[grid](args, BLOCK=...)` |\n",
    "| **Grid represents** | Number of **blocks** | Number of **programs** |\n",
    "| **Block/Thread control** | Explicit: `(tx, ty, tz)` per block | Abstracted: work on data blocks |\n",
    "| **Thread indexing** | Manual: `blockIdx`, `threadIdx` | Automatic: `tl.program_id()` + `tl.arange()` |\n",
    "| **Typical grid** | `(n_blocks_x, n_blocks_y, n_blocks_z)` | `(n_programs_x, n_programs_y, n_programs_z)` |\n",
    "| **Typical block** | `(threads_x, threads_y, threads_z)` | N/A (implicit in `BLOCK_SIZE`) |\n",
    "| **Memory access** | Per-thread indexing | Vectorized block operations |\n",
    "| **Abstraction level** | Low-level (like CUDA C) | High-level (compiler optimizes) |\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Example: Vector Addition\n",
    "\n",
    "### Numba CUDA Version:\n",
    "```python\n",
    "@cuda.jit\n",
    "def vector_add_numba(a, b, c, n):\n",
    "    i = cuda.grid(1)  # Global thread index\n",
    "    if i < n:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "# Launch\n",
    "n = 1_000_000\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n",
    "vector_add_numba[blocks_per_grid, threads_per_block](a, b, c, n)\n",
    "```\n",
    "\n",
    "### Triton Version:\n",
    "```python\n",
    "@triton.jit\n",
    "def vector_add_triton(a_ptr, b_ptr, c_ptr, n, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offset < n\n",
    "    \n",
    "    a = tl.load(a_ptr + offset, mask=mask)\n",
    "    b = tl.load(b_ptr + offset, mask=mask)\n",
    "    c = a + b\n",
    "    tl.store(c_ptr + offset, c, mask=mask)\n",
    "\n",
    "# Launch\n",
    "BLOCK_SIZE = 1024\n",
    "grid = (triton.cdiv(n, BLOCK_SIZE),)\n",
    "vector_add_triton[grid](a, b, c, n, BLOCK_SIZE=BLOCK_SIZE)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "- **Numba CUDA**: You think in terms of **blocks of threads** (2-level hierarchy: grid â†’ blocks â†’ threads)\n",
    "- **Triton**: You think in terms of **programs operating on data blocks** (1-level: grid â†’ programs, with implicit vectorization)\n",
    "\n",
    "Triton is conceptually **one level of \"blocks\"** in CUDA terms - each Triton program is roughly equivalent to a CUDA block, but Triton automatically handles the thread-level parallelism within that program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2781a3-9c37-483d-b087-ddc83c6d8365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-syst",
   "language": "python",
   "name": "cs336-syst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
