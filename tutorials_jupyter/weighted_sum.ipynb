{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcc3e40-4bf3-4635-bcba-085b813bec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16001b5a-9e44-4288-aac2-4481138f5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,  # Input pointers\n",
    "    output_ptr,  # Output pointer\n",
    "    x_stride_row, x_stride_dim,  # Strides tell us how to move one element in each axis of a tensor\n",
    "    weight_stride_dim,  # Likely 1\n",
    "    output_stride_row,  # Likely 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # Tile shapes must be known at compile time\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a tile of rows of x.\n",
    "    # `tl.program_id` gives us a way to check which thread block we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    \n",
    "    # Block pointers give us a way to select from an ND region of memory\n",
    "    # and move our selection around.\n",
    "    # The block pointer must know:\n",
    "    # - The pointer to the first element of the tensor\n",
    "    # - The overall shape of the tensor to handle out-of-bounds access\n",
    "    # - The strides of each dimension to use the memory layout properly\n",
    "    # - The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "    # - The block shape to use load/store at a time\n",
    "    # - The order of the dimensions in memory from major to minor\n",
    "    # axes (= np.argsort(strides)) for optimizations, especially useful on H100\n",
    "    \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    # Initialize a buffer to write to\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "    \n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        # Load the current block pointer\n",
    "        # Since ROWS_TILE_SIZE might not divide ROWS, and D_TILE_SIZE might not divide D,\n",
    "        # we need boundary checks for both dimensions\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "        \n",
    "        # Compute the weighted sum of the row.\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "        \n",
    "        # Move the pointers to the next tile.\n",
    "        # These are (rows, columns) coordinate deltas\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # Move by D_TILE_SIZE in the last dimension\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # Move by D_TILE_SIZE\n",
    "    \n",
    "    # Write output to the output block pointer (a single scalar per row).\n",
    "    # Since ROWS_TILE_SIZE might not divide ROWS, we need boundary checks\n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c073c7-e75c-4db6-9bf9-a9fe4e3d9692",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mWeightedSumFunc\u001b[39;00m(\u001b[43mtorch\u001b[49m.autograd.Function):\n\u001b[32m      2\u001b[39m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(ctx, x, weight):\n\u001b[32m      4\u001b[39m         \u001b[38;5;66;03m# Cache x and weight to be used in the backward pass, when we\u001b[39;00m\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# only receive the gradient wrt. the output tensor, and\u001b[39;00m\n\u001b[32m      6\u001b[39m         \u001b[38;5;66;03m# need to compute the gradients wrt. x and weight.\u001b[39;00m\n\u001b[32m      7\u001b[39m         D, output_dims = x.shape[-\u001b[32m1\u001b[39m], x.shape[:-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # Cache x and weight to be used in the backward pass, when we\n",
    "        # only receive the gradient wrt. the output tensor, and\n",
    "        # need to compute the gradients wrt. x and weight.\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "        \n",
    "        # Reshape input tensor to 2D\n",
    "        input_shape = x.shape\n",
    "        x = rearrange(x, \"... d -> (...) d\")\n",
    "        \n",
    "        ctx.save_for_backward(x, weight)\n",
    "        \n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
    "        assert x.is_contiguous(), \"Our pointer arithmetic will assume contiguous x\"\n",
    "        \n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16  # Roughly 16 loops through the embedding dimension\n",
    "        ctx.ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "        ctx.input_shape = input_shape\n",
    "        \n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "        y = torch.empty(output_dims, device=x.device)\n",
    "        \n",
    "        # Launch our kernel with n instances in our 1D grid.\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE, D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
    "        )\n",
    "        \n",
    "        return y.view(input_shape[:-1])\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        x, weight = ctx.saved_tensors\n",
    "        ROWS_TILE_SIZE, D_TILE_SIZE = ctx.ROWS_TILE_SIZE, ctx.D_TILE_SIZE  # These don't have to be the same\n",
    "        n_rows, D = x.shape\n",
    "        \n",
    "        # Our strategy is for each thread block to first write to a partial buffer,\n",
    "        # then we reduce over this buffer to get the final gradient.\n",
    "        partial_grad_weight = torch.empty((cdiv(n_rows, ROWS_TILE_SIZE), D), device=x.device, dtype=x.dtype)\n",
    "        grad_x = torch.empty_like(x)\n",
    "        \n",
    "        weighted_sum_backward[(cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            grad_out,\n",
    "            grad_x, partial_grad_weight,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            grad_out.stride(0),\n",
    "            grad_x.stride(0), grad_x.stride(1),\n",
    "            partial_grad_weight.stride(0), partial_grad_weight.stride(1),\n",
    "            NUM_ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "        grad_weight = partial_grad_weight.sum(axis=0)\n",
    "        return grad_x, grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51779549-1012-427b-8092-2e2c380af8cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'triton' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@triton\u001b[39m.jit\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweighted_sum_backward\u001b[39m(\n\u001b[32m      3\u001b[39m     x_ptr, weight_ptr,  \u001b[38;5;66;03m# Input\u001b[39;00m\n\u001b[32m      4\u001b[39m     grad_output_ptr,  \u001b[38;5;66;03m# Grad input\u001b[39;00m\n\u001b[32m      5\u001b[39m     grad_x_ptr, partial_grad_weight_ptr,  \u001b[38;5;66;03m# Grad outputs\u001b[39;00m\n\u001b[32m      6\u001b[39m     stride_xr, stride_xd,\n\u001b[32m      7\u001b[39m     stride_wd,\n\u001b[32m      8\u001b[39m     stride_gr,\n\u001b[32m      9\u001b[39m     stride_gxr, stride_gxd,\n\u001b[32m     10\u001b[39m     stride_gwb, stride_gwd,\n\u001b[32m     11\u001b[39m     NUM_ROWS, D,\n\u001b[32m     12\u001b[39m     ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,\n\u001b[32m     13\u001b[39m ):\n\u001b[32m     14\u001b[39m     row_tile_idx = tl.program_id(\u001b[32m0\u001b[39m)\n\u001b[32m     15\u001b[39m     n_row_tiles = tl.num_programs(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'triton' is not defined"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def weighted_sum_backward(\n",
    "    x_ptr, weight_ptr,  # Input\n",
    "    grad_output_ptr,  # Grad input\n",
    "    grad_x_ptr, partial_grad_weight_ptr,  # Grad outputs\n",
    "    stride_xr, stride_xd,\n",
    "    stride_wd,\n",
    "    stride_gr,\n",
    "    stride_gxr, stride_gxd,\n",
    "    stride_gwb, stride_gwd,\n",
    "    NUM_ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,\n",
    "):\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    n_row_tiles = tl.num_programs(0)\n",
    "    \n",
    "    # Inputs\n",
    "    grad_output_block_ptr = tl.make_block_ptr(\n",
    "        grad_output_ptr,\n",
    "        shape=(NUM_ROWS,), strides=(stride_gr,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(NUM_ROWS, D,), strides=(stride_xr, stride_xd),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,), strides=(stride_wd,),\n",
    "        offsets=(0,), block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    grad_x_block_ptr = tl.make_block_ptr(\n",
    "        grad_x_ptr,\n",
    "        shape=(NUM_ROWS, D,), strides=(stride_gxr, stride_gxd),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    partial_grad_weight_block_ptr = tl.make_block_ptr(\n",
    "        partial_grad_weight_ptr,\n",
    "        shape=(n_row_tiles, D,), strides=(stride_gwb, stride_gwd),\n",
    "        offsets=(row_tile_idx, 0),\n",
    "        block_shape=(1, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (ROWS_TILE_SIZE,)\n",
    "        \n",
    "        # Outer product for grad_x\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "        grad_x_row = grad_output[:, None] * weight[None, :]\n",
    "        tl.store(grad_x_block_ptr, grad_x_row, boundary_check=(0, 1))\n",
    "        \n",
    "        # Reduce as many rows as possible for the grad_weight result\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        grad_weight_row = tl.sum(row * grad_output[:, None], axis=0, keep_dims=True)\n",
    "        tl.store(partial_grad_weight_block_ptr, grad_weight_row, boundary_check=(1,))  # Never out of bounds for dim 0\n",
    "        \n",
    "        # Move the pointers to the next tile along D\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))\n",
    "        partial_grad_weight_block_ptr = partial_grad_weight_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        grad_x_block_ptr = grad_x_block_ptr.advance((0, D_TILE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9e397-19ca-4520-bab5-082526d1635e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290381d8-bd2e-4d4c-acf1-4c6c84a14ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-syst",
   "language": "python",
   "name": "cs336-syst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
