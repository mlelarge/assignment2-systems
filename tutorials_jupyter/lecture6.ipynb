{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c00b8d-82b1-4a26-8f31-97e16e778ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import ProfilerActivity\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import rearrange\n",
    "\n",
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def mean(x: list[float]) -> float:\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def check_equal(f1, f2):\n",
    "    x = torch.randn(2048, device=get_device())\n",
    "    y1 = f1(x)\n",
    "    y2 = f2(x)\n",
    "    assert torch.allclose(y1, y2, atol=1e-6)\n",
    "\n",
    "def check_equal2(f1, f2):\n",
    "    x = torch.randn(2048, 2048, device=get_device())\n",
    "    y1 = f1(x)\n",
    "    y2 = f2(x)\n",
    "    assert torch.allclose(y1, y2, atol=1e-6)\n",
    "    \n",
    "def run_operation1(dim: int, operation: Callable) -> Callable:\n",
    "    # Setup: create one random dim x dim matrices\n",
    "    x = torch.randn(dim, dim, device=get_device())\n",
    "    # Return a function to perform the operation\n",
    "    return lambda : operation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063b0615-a8df-4cc2-bd16-f42d21b83b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_softmax(x: torch.Tensor):\n",
    "    return torch.nn.functional.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def pytorch_gelu(x: torch.Tensor):\n",
    "    # Use the tanh approximation to match our implementation\n",
    "    return torch.nn.functional.gelu(x, approximate=\"tanh\")\n",
    "\n",
    "\n",
    "def manual_gelu(x: torch.Tensor):\n",
    "    return 0.5 * x * (1 + torch.tanh(0.79788456 * (x + 0.044715 * x * x * x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6503793e-2cad-49fa-ae0a-8739d125661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # Warmup: first times might be slower due to compilation, things not cached.\n",
    "    # Since we will run the kernel multiple times, the timing that matters is steady state.\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "\n",
    "    # Time it for real now!\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "    for trial in range(num_trials):  # Do it multiple times to capture variance\n",
    "        start_time = time.time()\n",
    "\n",
    "        run()  # Actually perform computation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "\n",
    "    mean_time = mean(times) # @inspect mean_time\n",
    "    return mean_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8527fa35-0e41-4f3f-a1d0-75ee804a51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_time = benchmark(\"manual_gelu\", run_operation1(dim=16384, operation=manual_gelu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9790bb-dcd5-4d0e-9152-2e761a4dedb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.559292475382487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fc4a186-8263-4bb7-90e3-ceb380209e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # Warmup\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "\n",
    "    # Run the code with the profiler\n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # Output stack trace for visualization\n",
    "            with_stack=with_stack,\n",
    "            # Needed to export stack trace for visualization\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "\n",
    "    # Print out table\n",
    "    table = prof.key_averages().table(sort_by=\"cuda_time_total\",\n",
    "                                      max_name_column_width=80,\n",
    "                                      row_limit=10)\n",
    "\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4ee3d6-8e9d-4bc1-90c4-ca79bb0d062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_gelu(x: torch.Tensor):\n",
    "    assert x.is_cuda\n",
    "    assert x.is_contiguous()\n",
    "\n",
    "    # Allocate output tensor\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # Determine grid (elements divided into blocks)\n",
    "    num_elements = x.numel()\n",
    "    block_size = 1024  # Number of threads\n",
    "    num_blocks = triton.cdiv(num_elements, block_size)\n",
    "\n",
    "    triton_gelu_kernel[(num_blocks,)](x, y, num_elements, BLOCK_SIZE=block_size)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def triton_gelu_kernel(x_ptr, y_ptr, num_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    # Input is at `x_ptr` and output is at `y_ptr`\n",
    "    #     |        Block 0            |          Block 1          |      ...      |\n",
    "    #                            BLOCK_SIZE                                 num_elements\n",
    "\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "\n",
    "    # Indices where this thread block should operate\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Handle boundary\n",
    "    mask = offsets < num_elements\n",
    "\n",
    "    # Read\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "\n",
    "    # Approx gelu is 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n",
    "    # Compute (tl.tanh doesn't exist, use tanh(a) = (exp(2a) - 1) / (exp(2a) + 1)\n",
    "    a = 0.79788456 * (x + 0.044715 * x * x * x)\n",
    "    exp = tl.exp(2 * a)\n",
    "    tanh = (exp - 1) / (exp + 1)\n",
    "    y = 0.5 * x * (1 + tanh)\n",
    "\n",
    "    # Store\n",
    "    tl.store(y_ptr + offsets, y, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee64945a-d545-4d1c-b11a-16936f9833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_time = benchmark(\"triton_gelu\", run_operation1(dim=16384, operation=triton_gelu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b83c155d-9dd1-4107-a96b-d77ec254a7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6663004557291666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a52503-2ef6-4697-a6df-6efcf56bc9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6324450174967448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_time = benchmark(\"pytorch_gelu\", run_operation1(dim=16384, operation=pytorch_gelu))\n",
    "pytorch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a6f6be-d44a-4551-8b4a-d31dc028d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_gelu_profile = profile(\"triton_gelu\", run_operation1(dim=16384, operation=triton_gelu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aec4878-c151-4da2-b701-e3cc6c637423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       triton_gelu_kernel         0.00%       0.000us         0.00%       0.000us       0.000us     600.614us       100.00%     600.614us     600.614us             1  \n",
      "         aten::empty_like        28.25%     614.633us        71.65%       1.559ms       1.559ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "      aten::empty_strided        43.41%     944.457us        43.41%     944.457us     944.457us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "           cuLaunchKernel         1.67%      36.325us         1.67%      36.325us      36.325us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        26.68%     580.431us        26.68%     580.431us     290.216us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.176ms\n",
      "Self CUDA time total: 600.614us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(triton_gelu_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599ad849-7ae7-428d-a60a-a4a59642b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_gelu = torch.compile(manual_gelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de9debde-a856-47da-88cf-7d8277856875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6775061289469401"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_time = benchmark(\"compiled_gelu\", run_operation1(dim=16384, operation=compiled_gelu))\n",
    "compiled_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd4e847-d842-41dd-aee1-adc23db81466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_softmax(x: torch.Tensor):\n",
    "    # M: number of rows, N: number of columns\n",
    "    M, N = x.shape\n",
    "\n",
    "    # Compute the max of each row (MN reads, M writes)\n",
    "    x_max = x.max(dim=1)[0]\n",
    "\n",
    "    # Subtract off the max (MN + M reads, MN writes)\n",
    "    x = x - x_max[:, None]\n",
    "\n",
    "    # Exponentiate (MN reads, MN writes)\n",
    "    numerator = torch.exp(x)\n",
    "\n",
    "    # Compute normalization constant (MN reads, M writes)\n",
    "    denominator = numerator.sum(dim=1)\n",
    "\n",
    "    # Normalize (MN reads, MN writes)\n",
    "    y = numerator / denominator[:, None]\n",
    "\n",
    "    # Total: 5MN + M reads, 3MN + 2M writes\n",
    "    # In principle, should have MN reads, MN writes (speedup of 4x!)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539624f6-96f8-4d77-97fb-25120a65f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_softmax = torch.compile(manual_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12449a50-4504-498d-ab88-06c7e1a05212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_softmax(x: torch.Tensor):\n",
    "    # Allocate output tensor\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # Determine grid\n",
    "    M, N = x.shape                          # Number of rows x number of columns\n",
    "    block_size = triton.next_power_of_2(N)  # Each block contains all the columns\n",
    "    num_blocks = M                          # Each block is a row\n",
    "\n",
    "    # Launch kernel\n",
    "    triton_softmax_kernel[(M,)](\n",
    "        x_ptr=x, y_ptr=y,\n",
    "        x_row_stride=x.stride(0), y_row_stride=y.stride(0),\n",
    "        num_cols=N, BLOCK_SIZE=block_size\n",
    "    )\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def triton_softmax_kernel(x_ptr, y_ptr, x_row_stride, y_row_stride, num_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    assert num_cols <= BLOCK_SIZE\n",
    "\n",
    "    # Process each row independently\n",
    "    row_idx = tl.program_id(0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Read from global memory\n",
    "    x_start_ptr = x_ptr + row_idx * x_row_stride\n",
    "    x_ptrs = x_start_ptr + col_offsets\n",
    "    x_row = tl.load(x_ptrs, mask=col_offsets < num_cols, other=float(\"-inf\"))\n",
    "\n",
    "    # Compute\n",
    "    x_row = x_row - tl.max(x_row, axis=0)\n",
    "    numerator = tl.exp(x_row)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    y_row = numerator / denominator\n",
    "\n",
    "    # Write back to global memory\n",
    "    y_start_ptr = y_ptr + row_idx * y_row_stride\n",
    "    y_ptrs = y_start_ptr + col_offsets\n",
    "    tl.store(y_ptrs, y_row, mask=col_offsets < num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c35c859-56a3-41e2-8c22-34b1be987b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_time = benchmark(\"manual_softmax\", run_operation1(dim=16384, operation=manual_softmax)) \n",
    "compiled_time = benchmark(\"compiled_softmax\", run_operation1(dim=16384, operation=compiled_softmax)) \n",
    "pytorch_time = benchmark(\"pytorch_softmax\", run_operation1(dim=16384, operation=pytorch_softmax)) \n",
    "triton_time = benchmark(\"triton_softmax\", run_operation1(dim=16384, operation=triton_softmax)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4eea825-eb42-4da4-8d01-19b7b6758339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual time: 3.1540393829345703\n",
      "compiled time: 0.7087389628092448\n",
      "pytorch time: 1.409769058227539\n",
      "triton time: 0.7029374440511068\n"
     ]
    }
   ],
   "source": [
    "print(f\"manual time: {manual_time}\")\n",
    "print(f\"compiled time: {compiled_time}\")\n",
    "print(f\"pytorch time: {pytorch_time}\")\n",
    "print(f\"triton time: {triton_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47a97e3e-05ad-4182-9df5-dc5f2ecf8531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16384, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 16384\n",
    "x = torch.randn(dim, dim, device=get_device())\n",
    "x.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbc3bf7f-855d-4260-944c-f9fec81b3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_softmax(x: torch.Tensor):\n",
    "    y = torch.empty_like(x)\n",
    "    M, N = x.shape\n",
    "    block_size = triton.next_power_of_2(N)\n",
    "    \n",
    "    triton_softmax_kernel[(M,)](\n",
    "        x, y, \n",
    "        M, N,\n",
    "        x.stride(0), y.stride(0),  # Pass strides from host\n",
    "        BLOCK_SIZE=block_size\n",
    "    )\n",
    "    return y\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def triton_softmax_kernel(\n",
    "    x_ptr, y_ptr, \n",
    "    M, N, \n",
    "    x_stride, y_stride,  # Receive strides as arguments\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    # Process each row independently\n",
    "    row_idx = tl.program_id(0)\n",
    "    \n",
    "    # Create block pointers for this row\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(M, N),\n",
    "        strides=(x_stride, 1),  # Use the passed stride\n",
    "        offsets=(row_idx, 0),\n",
    "        block_shape=(1, BLOCK_SIZE),\n",
    "        order=(1, 0)  # Row-major\n",
    "    )\n",
    "    \n",
    "    y_block_ptr = tl.make_block_ptr(\n",
    "        base=y_ptr,\n",
    "        shape=(M, N),\n",
    "        strides=(y_stride, 1),  # Use the passed stride\n",
    "        offsets=(row_idx, 0),\n",
    "        block_shape=(1, BLOCK_SIZE),\n",
    "        order=(1, 0)\n",
    "    )\n",
    "    \n",
    "    # Load row (automatically handles masking and padding)\n",
    "    x_row = tl.load(x_block_ptr, boundary_check=(1,), padding_option=\"zero\")\n",
    "    \n",
    "    # Compute softmax\n",
    "    x_row = x_row - tl.max(x_row)\n",
    "    numerator = tl.exp(x_row)\n",
    "    y_row = numerator / tl.sum(numerator)\n",
    "    \n",
    "    # Store result\n",
    "    tl.store(y_block_ptr, y_row, boundary_check=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1126e851-06e1-4643-a7af-931c4198482b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7072289784749349"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_time = benchmark(\"triton_softmax\", run_operation1(dim=16384, operation=triton_softmax))\n",
    "triton_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "287847f6-28a8-4606-bdfa-d2126db811d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_equal2(pytorch_softmax, triton_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b007ab2-c1be-4941-be25-7906652072bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,  # Input pointers\n",
    "    output_ptr,  # Output pointer\n",
    "    x_stride_row, x_stride_dim,  # Strides tell us how to move one element in each axis of a tensor\n",
    "    weight_stride_dim,  # Likely 1\n",
    "    output_stride_row,  # Likely 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # Tile shapes must be known at compile time\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a tile of rows of x.\n",
    "    # `tl.program_id` gives us a way to check which thread block we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    \n",
    "    # Block pointers give us a way to select from an ND region of memory\n",
    "    # and move our selection around.\n",
    "    # The block pointer must know:\n",
    "    # - The pointer to the first element of the tensor\n",
    "    # - The overall shape of the tensor to handle out-of-bounds access\n",
    "    # - The strides of each dimension to use the memory layout properly\n",
    "    # - The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "    # - The block shape to use load/store at a time\n",
    "    # - The order of the dimensions in memory from major to minor\n",
    "    # axes (= np.argsort(strides)) for optimizations, especially useful on H100\n",
    "    \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    # Initialize a buffer to write to\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "    \n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        # Load the current block pointer\n",
    "        # Since ROWS_TILE_SIZE might not divide ROWS, and D_TILE_SIZE might not divide D,\n",
    "        # we need boundary checks for both dimensions\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "        \n",
    "        # Compute the weighted sum of the row.\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "        \n",
    "        # Move the pointers to the next tile.\n",
    "        # These are (rows, columns) coordinate deltas\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # Move by D_TILE_SIZE in the last dimension\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # Move by D_TILE_SIZE\n",
    "    \n",
    "    # Write output to the output block pointer (a single scalar per row).\n",
    "    # Since ROWS_TILE_SIZE might not divide ROWS, we need boundary checks\n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a2b69d-f4b8-47dc-acdd-52c7cf1328f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_triton(x: torch.Tensor, weight: torch.Tensor):\n",
    "    D = x.shape[-1]\n",
    "    output_dims = x.shape[:-1]   \n",
    "    # Reshape input tensor to 2D\n",
    "    \n",
    "    x = rearrange(x, \"... d -> (...) d\")\n",
    "    # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "    y = torch.empty(x.shape[0], device=x.device)\n",
    "\n",
    "    D_TILE_SIZE = triton.next_power_of_2(D) // 16  # Roughly 16 loops through the embedding dimension\n",
    "    ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "        \n",
    "    # Launch our kernel with n instances in our 1D grid.\n",
    "    n_rows = y.numel()\n",
    "    weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "        \n",
    "    return y.view(output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf96a063-07f7-4d6a-8f3f-2d5ad5318385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal3(f1, f2):\n",
    "    x = torch.randn(64, 64, 2048, device=get_device())\n",
    "    w = torch.randn(2048, device=get_device())\n",
    "    y1 = f1(x,w)\n",
    "    y2 = f2(x,w)\n",
    "    #print(y1)\n",
    "    #print(y2)\n",
    "    assert torch.allclose(y1, y2, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9681f4eb-9e4d-46e1-a16f-e7bc25253b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_equal3(weighted_sum,weighted_sum_triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "508891ad-8ba4-430a-be55-846f498f0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # Cache x and weight to be used in the backward pass, when we\n",
    "        # only receive the gradient wrt. the output tensor, and\n",
    "        # need to compute the gradients wrt. x and weight.\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "        \n",
    "        # Reshape input tensor to 2D\n",
    "        x = rearrange(x, \"... d -> (...) d\")\n",
    "        \n",
    "        ctx.save_for_backward(x, weight)\n",
    "        \n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
    "        assert x.is_contiguous(), \"Our pointer arithmetic will assume contiguous x\"\n",
    "        \n",
    "        #ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16  # Roughly 16 loops through the embedding dimension\n",
    "        D_TILE_SIZE = triton.next_power_of_2(D) // 16  # Roughly 16 loops through the embedding dimension\n",
    "        #ctx.ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "        ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "        \n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "        y = torch.empty(x.shape[0], device=x.device)\n",
    "        \n",
    "        # Launch our kernel with n instances in our 1D grid.\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "        \n",
    "        return y.view(output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c310165-7c29-4d4c-8414-2e8469c0a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weightedsum = WeightedSumFunc.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c863e66d-8378-445e-81ab-8eaff333825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_equal3(weighted_sum,f_weightedsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0b5a078-7342-44a0-9246-d1343812fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weightedsum_compile = torch.compile(f_weightedsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d199002-9839-4118-a5d4-503e67b65820",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_equal3(weighted_sum,f_weightedsum_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "640141af-3fc7-46b0-891a-f33d167dab84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3647], device='cuda:0', grad_fn=<WeightedSumFuncBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 16, device=get_device())\n",
    "w = torch.randn(16, device=get_device()).requires_grad_(True)\n",
    "f_weightedsum_compile(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c984b-ea1f-4108-bbe7-f90f73614f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-syst",
   "language": "python",
   "name": "cs336-syst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
