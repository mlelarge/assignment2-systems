{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a73f95-eec8-4f36-8c6e-0a146c0ce8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA result: 140.000000\n",
      "NumPy result: 140.000000\n",
      "Match: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lelarge/miniconda3/envs/cs336-syst/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda, float32\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def dot_kernel_numba(a, b, out, size):\n",
    "    # ‚Üê At this point, a, b, out are ALREADY on the device (GPU)\n",
    "    # This kernel executes on the GPU\n",
    "    shared = cuda.shared.array(256, float32)\n",
    "    \n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    local_i = cuda.threadIdx.x\n",
    "    \n",
    "    if i < size:\n",
    "        shared[local_i] = a[i] * b[i]\n",
    "    else:\n",
    "        shared[local_i] = 0.0\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if local_i < stride:\n",
    "            shared[local_i] += shared[local_i + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "    \n",
    "    if local_i == 0:\n",
    "        cuda.atomic.add(out, 0, shared[0])\n",
    "\n",
    "\n",
    "def dot_cuda_numba(a, b):\n",
    "    \"\"\"Wrapper function - CPU code that manages device transfers\"\"\"\n",
    "    \n",
    "    # ========================================\n",
    "    # HOST ‚Üí DEVICE TRANSFERS (CPU ‚Üí GPU)\n",
    "    # ========================================\n",
    "    \n",
    "    # Transfer 'a' from CPU to GPU\n",
    "    a_device = cuda.to_device(a.astype(np.float32))  # ‚Üê DEVICE TRANSFER 1\n",
    "    \n",
    "    # Transfer 'b' from CPU to GPU  \n",
    "    b_device = cuda.to_device(b.astype(np.float32))  # ‚Üê DEVICE TRANSFER 2\n",
    "    \n",
    "    # Allocate 'out' directly on GPU (no transfer, just allocation)\n",
    "    out = cuda.to_device(np.zeros(1, dtype=np.float32))  # ‚Üê DEVICE ALLOCATION\n",
    "    \n",
    "    # ========================================\n",
    "    # KERNEL LAUNCH (runs on GPU)\n",
    "    # ========================================\n",
    "    \n",
    "    size = a_device.shape[0]\n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    # Launch kernel - a_device, b_device, out are all on GPU\n",
    "    dot_kernel_numba[blocks_per_grid, threads_per_block](\n",
    "        a_device, b_device, out, size\n",
    "    )\n",
    "    \n",
    "    # ========================================\n",
    "    # DEVICE ‚Üí HOST TRANSFER (GPU ‚Üí CPU)\n",
    "    # ========================================\n",
    "    \n",
    "    # Copy result back from GPU to CPU\n",
    "    return out.copy_to_host()  # ‚Üê DEVICE TRANSFER 3\n",
    "\n",
    "\n",
    "# Test\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "\n",
    "result = dot_cuda_numba(a, b)\n",
    "expected = np.dot(a, b)\n",
    "\n",
    "print(f\"CUDA result: {result[0]:.6f}\")\n",
    "print(f\"NumPy result: {expected:.6f}\")\n",
    "print(f\"Match: {np.allclose(result[0], expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afa0c78-945b-4f5d-a9d5-3fb304e33d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton result: 140.0\n",
      "PyTorch result: 140.0\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "@triton.jit\n",
    "def dot_kernel(\n",
    "    a_ptr,      # Pointer to first input vector\n",
    "    b_ptr,      # Pointer to second input vector  \n",
    "    out_ptr,    # Pointer to output scalar\n",
    "    size,       # Size of vectors\n",
    "    BLOCK_SIZE: tl.constexpr,  # Elements per program\n",
    "):\n",
    "    # Program ID (analogous to blockIdx.x)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Compute offsets for this program's block\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Mask for boundary handling\n",
    "    mask = offsets < size\n",
    "    \n",
    "    # Load data (coalesced, automatic)\n",
    "    a = tl.load(a_ptr + offsets, mask=mask, other=0.0)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask, other=0.0)\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    products = a * b\n",
    "    \n",
    "    # Reduce sum within this block (automatic parallel reduction!)\n",
    "    block_sum = tl.sum(products)\n",
    "    \n",
    "    # Atomic add to output (single thread per block does this)\n",
    "    tl.atomic_add(out_ptr, block_sum)\n",
    "\n",
    "\n",
    "def dot_triton(a, b):\n",
    "    \"\"\"Wrapper function to launch the kernel\"\"\"\n",
    "    # Allocate output\n",
    "    out = torch.zeros(1, device=a.device, dtype=a.dtype)\n",
    "    \n",
    "    # Grid and block configuration\n",
    "    size = a.shape[0]\n",
    "    BLOCK_SIZE = 256  # Triton works well with larger blocks\n",
    "    grid = lambda meta: (triton.cdiv(size, meta['BLOCK_SIZE']),)\n",
    "    \n",
    "    # Launch kernel\n",
    "    dot_kernel[grid](a, b, out, size, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# Usage example\n",
    "SIZE = 8\n",
    "a = torch.arange(SIZE, dtype=torch.float32, device='cuda')\n",
    "b = torch.arange(SIZE, dtype=torch.float32, device='cuda')\n",
    "\n",
    "result_triton = dot_triton(a, b)\n",
    "result_torch = torch.dot(a, b)\n",
    "\n",
    "print(f\"Triton result: {result_triton.item()}\")\n",
    "print(f\"PyTorch result: {result_torch.item()}\")\n",
    "print(f\"Match: {torch.allclose(result_triton, result_torch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b936e730-5287-499a-a3a0-280c2956f2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "\n",
      "============================================================\n",
      "GPU 0: NVIDIA H100 NVL\n",
      "============================================================\n",
      "\n",
      "üìä BASIC INFO:\n",
      "  Compute Capability: (9, 0)\n",
      "  PCI Device ID: 0\n",
      "\n",
      "üßµ THREAD LIMITS:\n",
      "  Max threads per block: 1024\n",
      "  Max block dimensions: 1024 √ó 1024 √ó 64\n",
      "  Max grid dimensions: 2147483647 √ó 65535 √ó 65535\n",
      "  Warp size: 32\n",
      "\n",
      "üíæ MEMORY:\n",
      "  Total global memory: 93.12 GB\n",
      "  Free memory: 91.82 GB\n",
      "  Used memory: 1.30 GB\n",
      "  Shared memory per block: 48.00 KB\n",
      "  Shared memory per SM: 228.00 KB\n",
      "  Constant memory: 64.00 KB\n",
      "\n",
      "üî¢ MULTIPROCESSORS:\n",
      "  Number of SMs: 132\n",
      "\n",
      "üìù REGISTERS:\n",
      "  Registers per block: 65536\n",
      "  Registers per SM: 65536\n",
      "\n",
      "‚ö° PERFORMANCE:\n",
      "  Clock rate: 1785.00 MHz\n",
      "  Memory clock rate: 2619.00 MHz\n",
      "  Memory bus width: 6144 bits\n",
      "  L2 cache size: 61440.00 KB\n",
      "\n",
      "‚ú® FEATURES:\n",
      "  Concurrent kernels: True\n",
      "  Unified addressing: True\n",
      "  ECC enabled: True\n",
      "  Managed memory: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "def show_gpu_info():\n",
    "    if not cuda.is_available():\n",
    "        print(\"No CUDA GPU available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Number of GPUs: {len(cuda.gpus)}\\n\")\n",
    "    \n",
    "    for i, gpu in enumerate(cuda.gpus):\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"GPU {i}: {gpu.name.decode('utf-8')}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        with cuda.gpus[i]:\n",
    "            device = cuda.get_current_device()\n",
    "            \n",
    "            # Helper function to safely get attribute\n",
    "            def safe_get(attr_name, default=\"N/A\"):\n",
    "                try:\n",
    "                    return getattr(device, attr_name)\n",
    "                except AttributeError:\n",
    "                    return default\n",
    "            \n",
    "            # Basic info\n",
    "            print(f\"\\nüìä BASIC INFO:\")\n",
    "            print(f\"  Compute Capability: {device.compute_capability}\")\n",
    "            print(f\"  PCI Device ID: {device.id}\")\n",
    "            \n",
    "            # Thread/Block limits\n",
    "            print(f\"\\nüßµ THREAD LIMITS:\")\n",
    "            print(f\"  Max threads per block: {safe_get('MAX_THREADS_PER_BLOCK')}\")\n",
    "            print(f\"  Max block dimensions: {safe_get('MAX_BLOCK_DIM_X')} √ó {safe_get('MAX_BLOCK_DIM_Y')} √ó {safe_get('MAX_BLOCK_DIM_Z')}\")\n",
    "            print(f\"  Max grid dimensions: {safe_get('MAX_GRID_DIM_X')} √ó {safe_get('MAX_GRID_DIM_Y')} √ó {safe_get('MAX_GRID_DIM_Z')}\")\n",
    "            print(f\"  Warp size: {safe_get('WARP_SIZE')}\")\n",
    "            \n",
    "            # Memory limits\n",
    "            print(f\"\\nüíæ MEMORY:\")\n",
    "            meminfo = cuda.current_context().get_memory_info()\n",
    "            free_memory = meminfo[0]\n",
    "            total_memory = meminfo[1]\n",
    "            print(f\"  Total global memory: {total_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"  Free memory: {free_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"  Used memory: {(total_memory - free_memory) / 1024**3:.2f} GB\")\n",
    "            \n",
    "            shared_mem = safe_get('MAX_SHARED_MEMORY_PER_BLOCK')\n",
    "            if shared_mem != \"N/A\":\n",
    "                print(f\"  Shared memory per block: {shared_mem / 1024:.2f} KB\")\n",
    "            \n",
    "            shared_mem_sm = safe_get('MAX_SHARED_MEMORY_PER_MULTIPROCESSOR')\n",
    "            if shared_mem_sm != \"N/A\":\n",
    "                print(f\"  Shared memory per SM: {shared_mem_sm / 1024:.2f} KB\")\n",
    "            \n",
    "            const_mem = safe_get('TOTAL_CONSTANT_MEMORY')\n",
    "            if const_mem != \"N/A\":\n",
    "                print(f\"  Constant memory: {const_mem / 1024:.2f} KB\")\n",
    "            \n",
    "            # Multiprocessor info\n",
    "            print(f\"\\nüî¢ MULTIPROCESSORS:\")\n",
    "            mp_count = safe_get('MULTIPROCESSOR_COUNT')\n",
    "            print(f\"  Number of SMs: {mp_count}\")\n",
    "            \n",
    "            max_threads_sm = safe_get('MAX_THREADS_PER_MULTIPROCESSOR')\n",
    "            if max_threads_sm != \"N/A\":\n",
    "                print(f\"  Max threads per SM: {max_threads_sm}\")\n",
    "                warp_size = safe_get('WARP_SIZE', 32)\n",
    "                if warp_size != \"N/A\":\n",
    "                    print(f\"  Max warps per SM: {max_threads_sm // warp_size}\")\n",
    "            \n",
    "            max_blocks_sm = safe_get('MAX_BLOCKS_PER_MULTIPROCESSOR')\n",
    "            if max_blocks_sm != \"N/A\":\n",
    "                print(f\"  Max blocks per SM: {max_blocks_sm}\")\n",
    "            \n",
    "            # Register info\n",
    "            print(f\"\\nüìù REGISTERS:\")\n",
    "            regs_block = safe_get('MAX_REGISTERS_PER_BLOCK')\n",
    "            if regs_block != \"N/A\":\n",
    "                print(f\"  Registers per block: {regs_block}\")\n",
    "            \n",
    "            regs_sm = safe_get('MAX_REGISTERS_PER_MULTIPROCESSOR')\n",
    "            if regs_sm != \"N/A\":\n",
    "                print(f\"  Registers per SM: {regs_sm}\")\n",
    "            \n",
    "            # Performance\n",
    "            print(f\"\\n‚ö° PERFORMANCE:\")\n",
    "            clock = safe_get('CLOCK_RATE')\n",
    "            if clock != \"N/A\":\n",
    "                print(f\"  Clock rate: {clock / 1000:.2f} MHz\")\n",
    "            \n",
    "            mem_clock = safe_get('MEMORY_CLOCK_RATE')\n",
    "            if mem_clock != \"N/A\":\n",
    "                print(f\"  Memory clock rate: {mem_clock / 1000:.2f} MHz\")\n",
    "            \n",
    "            mem_bus = safe_get('GLOBAL_MEMORY_BUS_WIDTH')\n",
    "            if mem_bus != \"N/A\":\n",
    "                print(f\"  Memory bus width: {mem_bus} bits\")\n",
    "            \n",
    "            l2_cache = safe_get('L2_CACHE_SIZE')\n",
    "            if l2_cache != \"N/A\":\n",
    "                print(f\"  L2 cache size: {l2_cache / 1024:.2f} KB\")\n",
    "            \n",
    "            # Features\n",
    "            print(f\"\\n‚ú® FEATURES:\")\n",
    "            concurrent = safe_get('CONCURRENT_KERNELS')\n",
    "            if concurrent != \"N/A\":\n",
    "                print(f\"  Concurrent kernels: {bool(concurrent)}\")\n",
    "            \n",
    "            unified = safe_get('UNIFIED_ADDRESSING')\n",
    "            if unified != \"N/A\":\n",
    "                print(f\"  Unified addressing: {bool(unified)}\")\n",
    "            \n",
    "            ecc = safe_get('ECC_ENABLED')\n",
    "            if ecc != \"N/A\":\n",
    "                print(f\"  ECC enabled: {bool(ecc)}\")\n",
    "            \n",
    "            managed = safe_get('MANAGED_MEMORY')\n",
    "            if managed != \"N/A\":\n",
    "                print(f\"  Managed memory: {bool(managed)}\")\n",
    "            \n",
    "            # Calculate theoretical occupancy if we have the data\n",
    "            if mp_count != \"N/A\" and max_threads_sm != \"N/A\":\n",
    "                print(f\"\\nüìà THEORETICAL LIMITS:\")\n",
    "                max_blocks = max_threads_sm // 256\n",
    "                print(f\"  Max blocks per SM (with 256 threads/block): {max_blocks}\")\n",
    "                total_threads = mp_count * max_threads_sm\n",
    "                print(f\"  Theoretical max concurrent threads: {total_threads:,}\")\n",
    "            \n",
    "        print()\n",
    "\n",
    "show_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359caf0-eba1-42a3-9410-ca037c23c444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-syst",
   "language": "python",
   "name": "cs336-syst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
